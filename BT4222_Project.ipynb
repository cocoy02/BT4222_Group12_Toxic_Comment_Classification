{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg2_TL3e6lOf"
      },
      "source": [
        "# Table of Content\n",
        "1. [Text Cleaning](#textcleaning)\n",
        "2. [Text Preprocessing](#textpreprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:15:40.459935Z",
          "start_time": "2023-03-09T05:15:40.446797Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J8voFUP6lOk",
        "outputId": "5045820d-18ed-4e80-caff-e1bb780b7617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 8.58 µs\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "# libraries for text cleaning\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# libraries and packages for text (pre-)processing \n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import *\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:13:29.548532Z",
          "start_time": "2023-03-09T05:13:28.196217Z"
        },
        "id": "gBVdsuOL6lOm",
        "outputId": "1ea0c0a0-68f4-45c9-b3b1-bbf67992a36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(159571, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"Data/train.csv\")\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_pY2Oud6lOm"
      },
      "source": [
        "<a id=\"textcleaning\"></a>\n",
        "# 1. Text Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBzMaVSL6lOn"
      },
      "source": [
        "## Convert to Lower Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvCX9VN26lOn"
      },
      "source": [
        "We convert all letters to lower case to prepare for the following steps of text cleaning. Exceptional cases such as capital abbreviation will be solved by replacing typos, slang, acronyms or informal abbreviations technique in the subsquent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtvId9rN6lOo",
        "outputId": "5032f3d0-98e3-45e2-98fd-d116f032911c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>explanation\\r\\nwhy the edits made under my use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>d'aww! he matches this background colour i'm s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>you, sir, are my hero. any chance you remember...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
              "0             0        0       0       0              0   \n",
              "1             0        0       0       0              0   \n",
              "2             0        0       0       0              0   \n",
              "3             0        0       0       0              0   \n",
              "4             0        0       0       0              0   \n",
              "\n",
              "                                          clean_text  \n",
              "0  explanation\\r\\nwhy the edits made under my use...  \n",
              "1  d'aww! he matches this background colour i'm s...  \n",
              "2  hey man, i'm really not trying to edit war. it...  \n",
              "3  \"\\r\\nmore\\r\\ni can't make any real suggestions...  \n",
              "4  you, sir, are my hero. any chance you remember...  "
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"clean_text\"] = train_df[\"comment_text\"].apply(lambda x: x.lower())\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAQ_vKu6lOo"
      },
      "source": [
        "## Expand Contractions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjaEJT1X6lOp"
      },
      "source": [
        "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. Removing contractions helps contribute to text standardization. We use contractions package to expand contractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-5332zZ6lOp"
      },
      "outputs": [],
      "source": [
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: contractions.fix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZuu_RWz6lOq",
        "outputId": "5758de09-ffe5-4a9d-a433-9b3148d2054a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: \n",
            " Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
            "Clean text: \n",
            " hey man, i am really not trying to edit war. it is just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\n"
          ]
        }
      ],
      "source": [
        "# check if expand contractions works\n",
        "print(\"Original text: \\n\", train_df[\"comment_text\"][2])\n",
        "print(\"Clean text: \\n\", train_df[\"clean_text\"][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDDjj6Yd6lOq"
      },
      "source": [
        "## Remove Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s537aUzz6lOr"
      },
      "source": [
        "Remove unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY2PrjNg6lOr"
      },
      "source": [
        "### Remove URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dEkw9uT6lOr"
      },
      "outputs": [],
      "source": [
        "# replace URL with space\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', ' ', x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz_zbwDL6lOr"
      },
      "source": [
        "###  Remove Non-ASCI Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvhlq6co6lOr"
      },
      "outputs": [],
      "source": [
        "# replace Non_ASCI characters with space\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(r'[^\\x00-\\x7f]', ' ', x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TULwYdZ_6lOs"
      },
      "source": [
        "###  Remove Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRe8qOFS6lOs"
      },
      "outputs": [],
      "source": [
        "regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        \"]+\", flags = re.UNICODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wABsaJiH6lOs"
      },
      "outputs": [],
      "source": [
        "# replace special characters with space\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: regrex_pattern.sub(' ', x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNOJbfEO6lOs",
        "outputId": "7eff2ec8-4539-4dea-df02-53aef089518b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: \n",
            " \"P.S. It's not polite to talk to people behind their backs, please remove your comments from Mrph's talk page.\r\n",
            "\r\n",
            "Vaughan\r\n",
            "You're right; I went to check your previous edit and found a page on the Marvel site that spelled it \"\"Vaughn\"\", but now I am finding many more that spell it correctly. Thanks for the edits.   (☎☓) \r\n",
            "\r\n",
            "\"\n",
            "Clean text: \n",
            " \"p.s. it is not polite to talk to people behind their backs, please remove your comments from mrph's talk page.\r\n",
            "\r\n",
            "vaughan\r\n",
            "you are right; i went to check your previous edit and found a page on the marvel site that spelled it \"\"vaughn\"\", but now i am finding many more that spell it correctly. thanks for the edits.   (  ) \r\n",
            "\r\n",
            "\"\n"
          ]
        }
      ],
      "source": [
        "# check if special characters are removed\n",
        "print(\"Original text: \\n\", train_df[\"comment_text\"][143])\n",
        "print(\"Clean text: \\n\", train_df[\"clean_text\"][143])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vGfsmkh6lOt"
      },
      "source": [
        "### Remove HTML Tag (BeautifulSoup not really useful? merely remove space?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0eV01hG6lOt"
      },
      "outputs": [],
      "source": [
        "cleaned_text = train_df[\"clean_text\"].apply(lambda x: BeautifulSoup(str(x)).get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZsyFPAE6lOt"
      },
      "outputs": [],
      "source": [
        "text_changed = cleaned_text!=train_df[\"clean_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkgNWgJj6lOt",
        "outputId": "3b3c64cf-6db7-4b14-c755-bc0d5692f6b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[228, 329, 3303, 3699, 3858, 4112, 4929, 5547, 5837, 6193]"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[i for i, x in enumerate(text_changed) if x][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4WcMVMS6lOt",
        "outputId": "fabe66a0-f4d1-47ee-a7f8-b892a15d48e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'   heritage from village           in macedonian          . sources claim that the village was pure slavic.'"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"clean_text\"][228]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rin-KW5_6lOt",
        "outputId": "85575126-ad60-4557-8a0a-5d74c72341fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'heritage from village           in macedonian          . sources claim that the village was pure slavic.'"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text[228]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqsIpux16lOt"
      },
      "outputs": [],
      "source": [
        "# replace HTML tag with space\n",
        "html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(html, \" \", x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMbjXAwP6lOu"
      },
      "source": [
        "###  Remove Extra Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOC4PGmJ6lOu"
      },
      "outputs": [],
      "source": [
        "# replace \\r\\n with space\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub('\\r\\n', ' ', x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLii6lnk6lOu"
      },
      "outputs": [],
      "source": [
        "# remove extra space\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(' +', ' ', x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68yB3AkH6lOu"
      },
      "source": [
        "## Replace Common Slangs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbEb5Lkj6lOu"
      },
      "source": [
        "Slang, acronyms or informal abbreviations should be replaced with formal English. The list of common slangs used in Tweets takes reference from https://www.kaggle.com/code/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeiaTG9q6lOu",
        "outputId": "65762560-59ce-4895-e0c4-c63595c36c8b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abbreviation</th>\n",
              "      <th>translation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$</td>\n",
              "      <td>dollar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>€</td>\n",
              "      <td>euro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4ao</td>\n",
              "      <td>for adults only</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a.m</td>\n",
              "      <td>before midday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a3</td>\n",
              "      <td>anytime anywhere anyplace</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  abbreviation                translation\n",
              "0            $                    dollar \n",
              "1            €                      euro \n",
              "2          4ao            for adults only\n",
              "3          a.m              before midday\n",
              "4           a3  anytime anywhere anyplace"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read abbreviation.csv\n",
        "abbreviations = pd.read_csv('Data/abbreviations.csv')\n",
        "abbreviations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDR-y7Bw6lOv"
      },
      "outputs": [],
      "source": [
        "# convert the data frame to a dictionary\n",
        "abbreviations_dict = dict(zip(abbreviations.abbreviation, abbreviations.translation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TAqz0A16lOv"
      },
      "outputs": [],
      "source": [
        "# define a helper function to replace the abbreviations\n",
        "def convert_abbrev(text):\n",
        "    # create a pattern of all abbreviations and make sure they are not part of a longer word\n",
        "    abbreviations_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in abbreviations_dict.keys()) + r')(?!\\w)')\n",
        "    # replace an abbreviation with its translation\n",
        "    text = abbreviations_pattern.sub(lambda x: abbreviations_dict[x.group()], text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt7rxipN6lOv"
      },
      "outputs": [],
      "source": [
        "# replace the slangs\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(convert_abbrev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap-nXTcz6lOv",
        "outputId": "f2e4517b-f415-4a02-b0fc-5aec91b38368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: \n",
            " D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
            "Clean text: \n",
            " d'aww! he matches this background colour i am seemingly stuck with. thanks. (talk) 21:51, january 11, 2016 (coordinated universal time)\n"
          ]
        }
      ],
      "source": [
        "# check if slangs are replaced\n",
        "print(\"Original text: \\n\", train_df[\"comment_text\"][1])\n",
        "print(\"Clean text: \\n\", train_df[\"clean_text\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emHZdEXQ6lOv"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2STK8pdp6lOv"
      },
      "source": [
        "We should correct the misspellings in the text. Both SpellChecker and TextBlob provide such functions, and we would like to compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZqlFmW-6lOw"
      },
      "outputs": [],
      "source": [
        "# select random texts from clean_text\n",
        "length = len(train_df[\"clean_text\"])\n",
        "random_num = random.sample(range(length), 100)\n",
        "random_text = train_df[\"clean_text\"][random_num]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQIDnE316lOw",
        "outputId": "bc74cb63-5169-4496-882f-2050b46a2ebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 133.19340062141418 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# using TextBlob package\n",
        "start_time1 = time.time()\n",
        "random_text.apply(lambda x: TextBlob(x).correct())\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkNiuhEN6lOw",
        "outputId": "b5d9ab68-5f20-46f4-f43f-2cb89d61aa40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 33.11142897605896 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# using SpellChecker package\n",
        "start_time2 = time.time()\n",
        "random_text.apply(lambda x: SpellChecker().correction(x))\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq8ZyOhp6lOw"
      },
      "source": [
        "Randomly select 100 texts and apply spelling correction functions on them. Comparing the execution time of 2 different packages, SpellChecker is much faster than TextBlob. Considering we are using a large-scale dataset, SpellChecker is preferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z86lgIkR6lOw"
      },
      "outputs": [],
      "source": [
        "def correct_spelling(text):\n",
        "    start_time = time.time()\n",
        "    cleaned_text = []\n",
        "    spellchecker = SpellChecker()\n",
        "    for i in range(text.shape[0]):\n",
        "        if i%100==0:\n",
        "            print(f'{i}-th text is being processed')\n",
        "        cleaned_text.append(spellchecker.correction(text[i]))\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG5votHY6lOx",
        "outputId": "90e75b6c-ae9b-4f09-ae26-84b028ac1f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0-th text is being processed\n",
            "100-th text is being processed\n",
            "200-th text is being processed\n",
            "300-th text is being processed\n",
            "400-th text is being processed\n",
            "500-th text is being processed\n",
            "600-th text is being processed\n",
            "700-th text is being processed\n",
            "800-th text is being processed\n",
            "900-th text is being processed\n",
            "1000-th text is being processed\n",
            "--- 124.26375722885132 seconds ---\n"
          ]
        }
      ],
      "source": [
        "cleaned_text = correct_spelling(train_df[\"clean_text\"][:1001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EHbF5xg6lOx",
        "outputId": "eea78c40-2a6a-4195-cae3-f74430fbd843"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Int64Index([ 62,  89, 101, 173, 175, 211, 217, 223, 226, 241, 250, 254, 259,\n",
              "            268, 276, 299, 320, 323, 376, 381, 397, 408, 423, 448, 465, 470,\n",
              "            504, 545, 592, 627, 632, 646, 715, 743, 758, 787, 806, 807, 814,\n",
              "            823, 831, 844, 852, 874, 877, 883, 897, 899, 913, 923, 947, 971],\n",
              "           dtype='int64')"
            ]
          },
          "execution_count": 254,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"clean_text\"][:1001].index[cleaned_text!=train_df[\"clean_text\"][:1001]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxxHMgpT6lOx",
        "outputId": "c11d4091-10dd-4924-aade-edbb2b114cf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'. fu ck ing trollreasons'"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"clean_text\"][971]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cv-gPsz6lOx"
      },
      "outputs": [],
      "source": [
        "cleaned_text[971]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_rHvPza6lOx"
      },
      "source": [
        "However, many corrections do not make sense, and may omit some useful information. We decided not to use established package to perform spelling correction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOQbOAg56lOx"
      },
      "source": [
        "## Remove Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DIYDhBP6lOx"
      },
      "source": [
        "We remove punctuations from the text as the final step of text cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IDueq1s6lOy"
      },
      "outputs": [],
      "source": [
        "# remove punctuations\n",
        "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3AW1Vuv6lOy",
        "outputId": "c5526244-eab7-4aba-f485-6ee4b6b99c61"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>daww he matches this background colour i am se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hey man i am really not trying to edit war it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>more i cannot make any real suggestions on im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
              "0             0        0       0       0              0   \n",
              "1             0        0       0       0              0   \n",
              "2             0        0       0       0              0   \n",
              "3             0        0       0       0              0   \n",
              "4             0        0       0       0              0   \n",
              "\n",
              "                                          clean_text  \n",
              "0  explanation why the edits made under my userna...  \n",
              "1  daww he matches this background colour i am se...  \n",
              "2  hey man i am really not trying to edit war it ...  \n",
              "3   more i cannot make any real suggestions on im...  \n",
              "4  you sir are my hero any chance you remember wh...  "
            ]
          },
          "execution_count": 268,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy4_l95Q6lOy"
      },
      "outputs": [],
      "source": [
        "train_df.drop('comment_text', axis=1).to_csv('Data/cleaned_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP1_2vo36lOy"
      },
      "source": [
        "<a id=\"textpreprocessing\"></a>\n",
        "# 2. Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:05:13.562077Z",
          "start_time": "2023-03-09T05:05:12.811551Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WeV9pWgO6lOy",
        "outputId": "a4bcd494-6401-492a-eda6-bf797c534e76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
              "0  0000997932d777bf      0             0        0       0       0   \n",
              "1  000103f0d9cfb60f      0             0        0       0       0   \n",
              "2  000113f07ec002fd      0             0        0       0       0   \n",
              "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
              "4  0001d958c54c6e35      0             0        0       0       0   \n",
              "\n",
              "   identity_hate                                         clean_text  \n",
              "0              0  explanation why the edits made under my userna...  \n",
              "1              0  daww he matches this background colour i am se...  \n",
              "2              0  hey man i am really not trying to edit war it ...  \n",
              "3              0   more i cannot make any real suggestions on im...  \n",
              "4              0  you sir are my hero any chance you remember wh...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf1ef1b7-cd5a-4c8a-a02e-d760f193f342\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>daww he matches this background colour i am se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hey man i am really not trying to edit war it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>more i cannot make any real suggestions on im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf1ef1b7-cd5a-4c8a-a02e-d760f193f342')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf1ef1b7-cd5a-4c8a-a02e-d760f193f342 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf1ef1b7-cd5a-4c8a-a02e-d760f193f342');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#cleaned_df = pd.read_csv(\"/cleaned_train.csv\")\n",
        "cleaned_df = pd.read_csv(\"Data/cleaned_train.csv\")\n",
        "cleaned_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnWPmf9h6lOz"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12No3DQD6lOz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi9Zr5yH6lOz"
      },
      "source": [
        "## Remove Stop Words (or/and Frequent words/ Rare words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2zjB4hb6lOz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7E0_XHB6lOz"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br2k1Vb_6lOz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekkah21N6lOz"
      },
      "source": [
        "## Part of Speech Tagging (POS Tagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xYltEgL6lO0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO6wgTNa6lO0"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:16:10.879978Z",
          "start_time": "2023-03-09T05:16:10.868387Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e65Rfmbr6lO0",
        "outputId": "9641a15f-beb2-48fa-8d06-eedf55673552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# stop = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:06:06.788926Z",
          "start_time": "2023-03-09T05:06:05.537963Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A3G-XVR6lO0",
        "outputId": "2e0641c8-3f22-41a1-e655-16ac26179aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now892053827\n",
            "after explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalism just closure on some gas after i voted at new york doll fac and please do not remove the template from the talk page since i am retired now892053827\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"before\",cleaned_df.iloc[0]['clean_text'])\n",
        "lm = [lemmatizer.lemmatize(t) for t in word_tokenize(cleaned_df.iloc[0]['clean_text'])]\n",
        "print(\"after\",\" \".join(lm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:38:35.700669Z",
          "start_time": "2023-03-09T05:38:35.616064Z"
        },
        "id": "U-IL5wa96lO0"
      },
      "outputs": [],
      "source": [
        "def process(sentence, stop):\n",
        "    wordsList = nltk.word_tokenize(sentence)\n",
        " \n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop]\n",
        " \n",
        "    #  Using a Tagger. Which is part-of-speech\n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "    pos_tagged_text = [ word for word, tag in tagged if tag.startswith('JJ') or tag.startswith('NN') or tag.startswith('RB') or tag.startswith('VB')]\n",
        "    return pos_tagged_text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-03-09T05:39:24.174Z"
        },
        "id": "dXoxUGix6lO0"
      },
      "outputs": [],
      "source": [
        "cleaned_df['lemmatization'] = cleaned_df['clean_text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(t) for t in process(x, stop)]))\n",
        "#cleaned_df['lemmatization_w_pos'] = cleaned_df['taged_words'].apply(lambda x: \" \".join([lemmatizer.lemmatize(t) for t in word_tokenize(x)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T03:37:14.085902Z",
          "start_time": "2023-03-09T03:37:14.080385Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD92dGuQ6lO0",
        "outputId": "43fe9e7f-16e2-43c2-81d8-9d9c360fcce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before  congratulations from me as well use the tools well talk \n",
            "after congratulation well tool well talk\n"
          ]
        }
      ],
      "source": [
        "print(\"before\",cleaned_df.iloc[5]['clean_text'])\n",
        "print(\"after\",cleaned_df.iloc[5]['lemmatization'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uba5KcyE6lO1"
      },
      "source": [
        "## Weighted Words - Bag of Words (BoW) - Bag of n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency Vector"
      ],
      "metadata": {
        "id": "VTtIaSNjQ571"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:07:42.198387Z",
          "start_time": "2023-03-09T05:07:42.194562Z"
        },
        "id": "g8ZK10iB6lO1"
      },
      "outputs": [],
      "source": [
        "def bagging(ngram,cleaned_df):\n",
        "  count_vec = CountVectorizer(ngram_range = (ngram, ngram))\n",
        "  weight_words = count_vec.fit_transform(cleaned_df['lemmatization'])\n",
        "  return pd.DataFrame(weight_words.toarray().transpose(), index=count_vec.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-03-09T05:07:51.433515Z",
          "start_time": "2023-03-09T05:07:44.080948Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "toH_aJKA6lO1",
        "outputId": "0b8cf0ca-4abb-415f-d044-812566e1fd26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f99208d33eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muni_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muni_gram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/BOW_Unigram.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbi_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbi_gram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/BOW_Bigram.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtri_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m         )\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m             )\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "uni_gram = bagging(1, cleaned_df)\n",
        "uni_gram.to_csv(\"/BOW_Unigram.csv\")\n",
        "bi_gram = bagging(2, cleaned_df)\n",
        "bi_gram.to_csv(\"/BOW_Bigram.csv\")\n",
        "tri_gram = bagging(3, cleaned_df)\n",
        "tri_gram.to_csv(\"/BOW_Trigram.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "XcKLU4DFQ-tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TF_IDF(ngram, cleaned_df):\n",
        "    tfidf = TfidfVectorizer(ngram_range = (ngram, ngram))\n",
        "    weight_words = tfidf.fit_transform(cleaned_df['lemmatization'])\n",
        "    return pd.DataFrame(weight_words.toarray().transpose(), index=tfidf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "0Qva3jj-REVB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uni_gram = TF_IDF(1, cleaned_df)\n",
        "# uni_gram.to_csv(\"/TF_IDF_Unigram.csv\")\n",
        "# bi_gram = TF_IDF(2, cleaned_df)\n",
        "# bi_gram.to_csv(\"/TF_IDF_Bigram.csv\")\n",
        "# tri_gram = TF_IDF(3, cleaned_df)\n",
        "# tri_gram.to_csv(\"/TF_IDF_Trigram.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "5vSKsKakRn2k",
        "outputId": "fb2fb140-17d5-4ad1-fba1-8c8db9d35e10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'lemmatization'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-daca5a9e70cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muni_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# uni_gram.to_csv(\"/TF_IDF_Unigram.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# bi_gram = TF_IDF(2, cleaned_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# bi_gram.to_csv(\"/TF_IDF_Bigram.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tri_gram = TF_IDF(3, cleaned_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c099bd42c684>\u001b[0m in \u001b[0;36mTF_IDF\u001b[0;34m(ngram, cleaned_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mweight_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'lemmatization'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}