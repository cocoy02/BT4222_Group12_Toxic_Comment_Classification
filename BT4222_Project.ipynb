{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg2_TL3e6lOf"
   },
   "source": [
    "# Table of Content\n",
    "1. [Text Cleaning](#textcleaning)\n",
    "2. [Text Preprocessing](#textpreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:15:40.459935Z",
     "start_time": "2023-03-09T05:15:40.446797Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J8voFUP6lOk",
    "outputId": "5045820d-18ed-4e80-caff-e1bb780b7617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# libraries for text cleaning\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# libraries and packages for text (pre-)processing \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:13:29.548532Z",
     "start_time": "2023-03-09T05:13:28.196217Z"
    },
    "id": "gBVdsuOL6lOm",
    "outputId": "1ea0c0a0-68f4-45c9-b3b1-bbf67992a36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_pY2Oud6lOm"
   },
   "source": [
    "<a id=\"textcleaning\"></a>\n",
    "# 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBzMaVSL6lOn"
   },
   "source": [
    "## Convert to Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvCX9VN26lOn"
   },
   "source": [
    "We convert all letters to lower case to prepare for the following steps of text cleaning. Exceptional cases such as capital abbreviation will be solved by replacing typos, slang, acronyms or informal abbreviations technique in the subsquent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wtvId9rN6lOo",
    "outputId": "5032f3d0-98e3-45e2-98fd-d116f032911c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  explanation\\nwhy the edits made under my usern...  \n",
       "1  d'aww! he matches this background colour i'm s...  \n",
       "2  hey man, i'm really not trying to edit war. it...  \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...  \n",
       "4  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"] = train_df[\"comment_text\"].apply(lambda x: x.lower())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geAQ_vKu6lOo"
   },
   "source": [
    "## Expand Contractions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjaEJT1X6lOp"
   },
   "source": [
    "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. Removing contractions helps contribute to text standardization. We use contractions package to expand contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S-5332zZ6lOp"
   },
   "outputs": [],
   "source": [
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vZuu_RWz6lOq",
    "outputId": "5758de09-ffe5-4a9d-a433-9b3148d2054a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "Clean text: \n",
      " hey man, i am really not trying to edit war. it is just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\n"
     ]
    }
   ],
   "source": [
    "# check if expand contractions works\n",
    "print(\"Original text: \\n\", train_df[\"comment_text\"][2])\n",
    "print(\"Clean text: \\n\", train_df[\"clean_text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDDjj6Yd6lOq"
   },
   "source": [
    "## Remove Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s537aUzz6lOr"
   },
   "source": [
    "Remove unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY2PrjNg6lOr"
   },
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5dEkw9uT6lOr"
   },
   "outputs": [],
   "source": [
    "# replace URL with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz_zbwDL6lOr"
   },
   "source": [
    "###  Remove Non-ASCI Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yvhlq6co6lOr"
   },
   "outputs": [],
   "source": [
    "# replace Non_ASCI characters with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(r'[^\\x00-\\x7f]', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TULwYdZ_6lOs"
   },
   "source": [
    "###  Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qRe8qOFS6lOs"
   },
   "outputs": [],
   "source": [
    "regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        \"]+\", flags = re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wABsaJiH6lOs"
   },
   "outputs": [],
   "source": [
    "# replace special characters with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: regrex_pattern.sub(' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zNOJbfEO6lOs",
    "outputId": "7eff2ec8-4539-4dea-df02-53aef089518b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " \"P.S. It's not polite to talk to people behind their backs, please remove your comments from Mrph's talk page.\n",
      "\n",
      "Vaughan\n",
      "You're right; I went to check your previous edit and found a page on the Marvel site that spelled it \"\"Vaughn\"\", but now I am finding many more that spell it correctly. Thanks for the edits.   (☎☓) \n",
      "\n",
      "\"\n",
      "Clean text: \n",
      " \"p.s. it is not polite to talk to people behind their backs, please remove your comments from mrph's talk page.\n",
      "\n",
      "vaughan\n",
      "you are right; i went to check your previous edit and found a page on the marvel site that spelled it \"\"vaughn\"\", but now i am finding many more that spell it correctly. thanks for the edits.   (  ) \n",
      "\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# check if special characters are removed\n",
    "print(\"Original text: \\n\", train_df[\"comment_text\"][143])\n",
    "print(\"Clean text: \\n\", train_df[\"clean_text\"][143])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vGfsmkh6lOt"
   },
   "source": [
    "### Remove HTML Tag (BeautifulSoup not really useful? merely remove space?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V0eV01hG6lOt"
   },
   "outputs": [],
   "source": [
    "cleaned_text = train_df[\"clean_text\"].apply(lambda x: BeautifulSoup(str(x)).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mZsyFPAE6lOt"
   },
   "outputs": [],
   "source": [
    "text_changed = cleaned_text!=train_df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dkgNWgJj6lOt",
    "outputId": "3b3c64cf-6db7-4b14-c755-bc0d5692f6b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[228, 329, 3303, 3699, 3858, 4112, 4929, 5547, 5837, 6193]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i, x in enumerate(text_changed) if x][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d4WcMVMS6lOt",
    "outputId": "fabe66a0-f4d1-47ee-a7f8-b892a15d48e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   heritage from village           in macedonian          . sources claim that the village was pure slavic.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"][228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rin-KW5_6lOt",
    "outputId": "85575126-ad60-4557-8a0a-5d74c72341fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heritage from village           in macedonian          . sources claim that the village was pure slavic.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QqsIpux16lOt"
   },
   "outputs": [],
   "source": [
    "# replace HTML tag with space\n",
    "html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(html, \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMbjXAwP6lOu"
   },
   "source": [
    "###  Remove Extra Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dOC4PGmJ6lOu"
   },
   "outputs": [],
   "source": [
    "# replace \\r\\n with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub('\\r\\n', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CLii6lnk6lOu"
   },
   "outputs": [],
   "source": [
    "# remove extra space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68yB3AkH6lOu"
   },
   "source": [
    "## Replace Common Slangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbEb5Lkj6lOu"
   },
   "source": [
    "Slang, acronyms or informal abbreviations should be replaced with formal English. The list of common slangs used in Tweets takes reference from https://www.kaggle.com/code/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IeiaTG9q6lOu",
    "outputId": "65762560-59ce-4895-e0c4-c63595c36c8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$</td>\n",
       "      <td>dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>€</td>\n",
       "      <td>euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ao</td>\n",
       "      <td>for adults only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a.m</td>\n",
       "      <td>before midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3</td>\n",
       "      <td>anytime anywhere anyplace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abbreviation                translation\n",
       "0            $                    dollar \n",
       "1            €                      euro \n",
       "2          4ao            for adults only\n",
       "3          a.m              before midday\n",
       "4           a3  anytime anywhere anyplace"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read abbreviation.csv\n",
    "abbreviations = pd.read_csv('Data/abbreviations.csv')\n",
    "abbreviations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tDR-y7Bw6lOv"
   },
   "outputs": [],
   "source": [
    "# convert the data frame to a dictionary\n",
    "abbreviations_dict = dict(zip(abbreviations.abbreviation, abbreviations.translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_TAqz0A16lOv"
   },
   "outputs": [],
   "source": [
    "# define a helper function to replace the abbreviations\n",
    "def convert_abbrev(text):\n",
    "    # create a pattern of all abbreviations and make sure they are not part of a longer word\n",
    "    abbreviations_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in abbreviations_dict.keys()) + r')(?!\\w)')\n",
    "    # replace an abbreviation with its translation\n",
    "    text = abbreviations_pattern.sub(lambda x: abbreviations_dict[x.group()], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vt7rxipN6lOv"
   },
   "outputs": [],
   "source": [
    "# replace the slangs\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(convert_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ap-nXTcz6lOv",
    "outputId": "f2e4517b-f415-4a02-b0fc-5aec91b38368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "Clean text: \n",
      " d'aww! he matches this background colour i am seemingly stuck with. thanks. (talk) 21:51, january 11, 2016 (coordinated universal time)\n"
     ]
    }
   ],
   "source": [
    "# check if slangs are replaced\n",
    "print(\"Original text: \\n\", train_df[\"comment_text\"][1])\n",
    "print(\"Clean text: \\n\", train_df[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emHZdEXQ6lOv"
   },
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2STK8pdp6lOv"
   },
   "source": [
    "We should correct the misspellings in the text. Both SpellChecker and TextBlob provide such functions, and we would like to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IZqlFmW-6lOw"
   },
   "outputs": [],
   "source": [
    "# select random texts from clean_text\n",
    "length = len(train_df[\"clean_text\"])\n",
    "random_num = random.sample(range(length), 100)\n",
    "random_text = train_df[\"clean_text\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BQIDnE316lOw",
    "outputId": "bc74cb63-5169-4496-882f-2050b46a2ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 50.41714692115784 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# using TextBlob package\n",
    "start_time1 = time.time()\n",
    "random_text.apply(lambda x: TextBlob(x).correct())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WkNiuhEN6lOw",
    "outputId": "b5d9ab68-5f20-46f4-f43f-2cb89d61aa40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 15.755198955535889 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# using SpellChecker package\n",
    "start_time2 = time.time()\n",
    "random_text.apply(lambda x: SpellChecker().correction(x))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq8ZyOhp6lOw"
   },
   "source": [
    "Randomly select 100 texts and apply spelling correction functions on them. Comparing the execution time of 2 different packages, SpellChecker is much faster than TextBlob. Considering we are using a large-scale dataset, SpellChecker is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "z86lgIkR6lOw"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    start_time = time.time()\n",
    "    cleaned_text = []\n",
    "    spellchecker = SpellChecker()\n",
    "    for i in range(text.shape[0]):\n",
    "        if i%100==0:\n",
    "            print(f'{i}-th text is being processed')\n",
    "        cleaned_text.append(spellchecker.correction(text[i]))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VG5votHY6lOx",
    "outputId": "90e75b6c-ae9b-4f09-ae26-84b028ac1f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th text is being processed\n",
      "100-th text is being processed\n",
      "200-th text is being processed\n",
      "300-th text is being processed\n",
      "400-th text is being processed\n",
      "500-th text is being processed\n",
      "600-th text is being processed\n",
      "700-th text is being processed\n",
      "800-th text is being processed\n",
      "900-th text is being processed\n",
      "1000-th text is being processed\n",
      "--- 103.30804991722107 seconds ---\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = correct_spelling(train_df[\"clean_text\"][:1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1EHbF5xg6lOx",
    "outputId": "eea78c40-2a6a-4195-cae3-f74430fbd843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 62,  89, 101, 173, 175, 211, 217, 223, 226, 241, 250, 254, 259,\n",
       "            268, 276, 299, 320, 323, 376, 381, 397, 408, 423, 448, 465, 470,\n",
       "            504, 545, 592, 627, 632, 646, 715, 743, 758, 787, 806, 807, 814,\n",
       "            823, 831, 844, 852, 874, 877, 883, 897, 899, 913, 923, 947, 971],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"][:1001].index[cleaned_text!=train_df[\"clean_text\"][:1001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fxxHMgpT6lOx",
    "outputId": "c11d4091-10dd-4924-aade-edbb2b114cf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. fu ck ing trollreasons'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"][971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3Cv-gPsz6lOx"
   },
   "outputs": [],
   "source": [
    "cleaned_text[971]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_rHvPza6lOx"
   },
   "source": [
    "However, many corrections do not make sense, and may omit some useful information. We decided not to use established package to perform spelling correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########  for POS Tagging use  ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"for_tagging_use\"] = train_df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOQbOAg56lOx"
   },
   "source": [
    "## Remove Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DIYDhBP6lOx"
   },
   "source": [
    "We remove punctuations from the text as the final step of text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5IDueq1s6lOy"
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "s3AW1Vuv6lOy",
    "outputId": "c5526244-eab7-4aba-f485-6ee4b6b99c61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>for_tagging_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  daww he matches this background colour i am se...   \n",
       "2  hey man i am really not trying to edit war it ...   \n",
       "3  \\nmore\\ni cannot make any real suggestions on ...   \n",
       "4  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                     for_tagging_use  \n",
       "0  explanation\\nwhy the edits made under my usern...  \n",
       "1  d'aww! he matches this background colour i am ...  \n",
       "2  hey man, i am really not trying to edit war. i...  \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...  \n",
       "4  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hy4_l95Q6lOy"
   },
   "outputs": [],
   "source": [
    "train_df.drop('comment_text', axis=1).to_csv('Data/cleaned_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP1_2vo36lOy"
   },
   "source": [
    "<a id=\"textpreprocessing\"></a>\n",
    "# 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:05:13.562077Z",
     "start_time": "2023-03-09T05:05:12.811551Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WeV9pWgO6lOy",
    "outputId": "a4bcd494-6401-492a-eda6-bf797c534e76",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>for_tagging_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation\\nwhy the edits made under my usern...   \n",
       "1              0  daww he matches this background colour i am se...   \n",
       "2              0  hey man i am really not trying to edit war it ...   \n",
       "3              0  \\nmore\\ni cannot make any real suggestions on ...   \n",
       "4              0  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                     for_tagging_use  \n",
       "0  explanation\\nwhy the edits made under my usern...  \n",
       "1  d'aww! he matches this background colour i am ...  \n",
       "2  hey man, i am really not trying to edit war. i...  \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...  \n",
       "4  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned_df = pd.read_csv(\"/cleaned_train.csv\")\n",
    "cleaned_df = pd.read_csv(\"Data/cleaned_train.csv\")\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnWPmf9h6lOz"
   },
   "source": [
    "## Tokenization (NLTK has a package specially catered to tokenizing tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "cleaned_df['text_tokenized'] = cleaned_df['clean_text'].apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>for_tagging_use</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, i, am, really, not, trying, to, edi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>[more, i, cannot, make, any, real, suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation\\nwhy the edits made under my usern...   \n",
       "1              0  daww he matches this background colour i am se...   \n",
       "2              0  hey man i am really not trying to edit war it ...   \n",
       "3              0  \\nmore\\ni cannot make any real suggestions on ...   \n",
       "4              0  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                     for_tagging_use  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i am ...   \n",
       "2  hey man, i am really not trying to edit war. i...   \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [daww, he, matches, this, background, colour, ...  \n",
       "2  [hey, man, i, am, really, not, trying, to, edi...  \n",
       "3  [more, i, cannot, make, any, real, suggestions...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi9Zr5yH6lOz"
   },
   "source": [
    "## Remove Stop Words (or/and Frequent words/ Rare words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "J2zjB4hb6lOz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#review all stop words in the library\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>for_tagging_use</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>for_stemming_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, i, am, really, not, trying, to, edi...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nmore\\ni cannot make any real suggestions on ...</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>[more, i, cannot, make, any, real, suggestions...</td>\n",
       "      <td>cannot make real suggestions improvement wonde...</td>\n",
       "      <td>[cannot, make, real, suggestions, improvement,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation\\nwhy the edits made under my usern...   \n",
       "1              0  daww he matches this background colour i am se...   \n",
       "2              0  hey man i am really not trying to edit war it ...   \n",
       "3              0  \\nmore\\ni cannot make any real suggestions on ...   \n",
       "4              0  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                     for_tagging_use  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i am ...   \n",
       "2  hey man, i am really not trying to edit war. i...   \n",
       "3  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [daww, he, matches, this, background, colour, ...   \n",
       "2  [hey, man, i, am, really, not, trying, to, edi...   \n",
       "3  [more, i, cannot, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                  text_no_stop_words  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  daww matches background colour seemingly stuck...   \n",
       "2  hey man really trying edit war guy constantly ...   \n",
       "3  cannot make real suggestions improvement wonde...   \n",
       "4                      sir hero chance remember page   \n",
       "\n",
       "                                    for_stemming_use  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [daww, matches, background, colour, seemingly,...  \n",
       "2  [hey, man, really, trying, edit, war, guy, con...  \n",
       "3  [cannot, make, real, suggestions, improvement,...  \n",
       "4                [sir, hero, chance, remember, page]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words from tokenized text\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    # check in lowercase \n",
    "    t = [token for token in row['text_tokenized'] if token.lower() not in stop_words]\n",
    "    text = ' '.join(t)    \n",
    "    return pd.Series([text,t])    #for stemming\n",
    "\n",
    "cleaned_df[['text_no_stop_words','for_stemming_use']] = cleaned_df.apply(remove_stopwords, axis = 1)\n",
    "\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>for_tagging_use</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>for_stemming_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>cannot make real suggestions improvement wonde...</td>\n",
       "      <td>[cannot, make, real, suggestions, improvement,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                    for_tagging_use  \\\n",
       "0              0  explanation\\nwhy the edits made under my usern...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                  text_no_stop_words  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  daww matches background colour seemingly stuck...   \n",
       "2  hey man really trying edit war guy constantly ...   \n",
       "3  cannot make real suggestions improvement wonde...   \n",
       "4                      sir hero chance remember page   \n",
       "\n",
       "                                    for_stemming_use  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [daww, matches, background, colour, seemingly,...  \n",
       "2  [hey, man, really, trying, edit, war, guy, con...  \n",
       "3  [cannot, make, real, suggestions, improvement,...  \n",
       "4                [sir, hero, chance, remember, page]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop clean_text and tokenized column\n",
    "del cleaned_df['clean_text']\n",
    "del cleaned_df['text_tokenized']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Words Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Word  Frequency\n",
      "113       article      55457\n",
      "20           page      45652\n",
      "124         would      36217\n",
      "159     wikipedia      35679\n",
      "19           talk      31768\n",
      "16         please      29617\n",
      "129           one      28098\n",
      "296          like      27708\n",
      "38           time      22426\n",
      "249           see      21579\n",
      "163          also      20550\n",
      "70          think      20039\n",
      "85           know      19171\n",
      "841        people      17794\n",
      "43           edit      17597\n",
      "88       articles      17524\n",
      "104           use      16326\n",
      "72            may      15559\n",
      "31         thanks      13902\n",
      "422          even      13393\n",
      "472           get      13384\n",
      "59           make      12943\n",
      "379         could      12856\n",
      "270          good      12743\n",
      "49    information      12407\n",
      "1041          way      12090\n",
      "83           want      11913\n",
      "409         point      11900\n",
      "103          well      11447\n",
      "588       sources      11264\n",
      "1343         name      10998\n",
      "175         pages      10998\n",
      "198      deletion      10860\n",
      "80          first      10779\n",
      "12            new      10574\n",
      "1256         help      10490\n",
      "437       editing      10219\n",
      "205        source      10191\n",
      "73           need      10188\n",
      "168            go      10177\n",
      "1147          say      10077\n",
      "64        section      10001\n",
      "1           edits       9906\n",
      "202         thank       9803\n",
      "2450         user       9695\n",
      "2            made       9568\n",
      "441          many       9547\n",
      "644          much       9432\n",
      "57            can       9259\n",
      "58            not       9259\n"
     ]
    }
   ],
   "source": [
    "#identify frequent words\n",
    "from nltk.probability import FreqDist\n",
    "combined_text_no_stop_words = ' '.join(cleaned_df['text_no_stop_words'])\n",
    "\n",
    "def most_frequent_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    fdist = FreqDist(words) \n",
    "    \n",
    "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
    "                             'Frequency': fdist.values()})\n",
    "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    return df_fdist.head(50)\n",
    "\n",
    "most_frequent_word_df = most_frequent_words(combined_text_no_stop_words)\n",
    "most_frequent_word_list = list(most_frequent_words(combined_text_no_stop_words)['Word'])\n",
    "\n",
    "print(most_frequent_word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7E0_XHB6lOz"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br2k1Vb_6lOz"
   },
   "source": [
    "We will use Snowball Stemmer to realise stemming. Comparing with Poster Stemmer, it is more efficient and has higher performance. Comparing with Lancaster Stemmer, it is less aggressive and can keep more word meanings for the Semantic Analysis in our later stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>for_tagging_use</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>daww match background colour seem stuck thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>hey man realli tri edit war guy constant remov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>cannot make real suggestions improvement wonde...</td>\n",
       "      <td>cannot make real suggest improv wonder section...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>sir hero chanc rememb page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                    for_tagging_use  \\\n",
       "0              0  explanation\\nwhy the edits made under my usern...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                  text_no_stop_words  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  daww matches background colour seemingly stuck...   \n",
       "2  hey man really trying edit war guy constantly ...   \n",
       "3  cannot make real suggestions improvement wonde...   \n",
       "4                      sir hero chance remember page   \n",
       "\n",
       "                                             stemmed  \n",
       "0  explan edit made usernam hardcor metallica fan...  \n",
       "1  daww match background colour seem stuck thank ...  \n",
       "2  hey man realli tri edit war guy constant remov...  \n",
       "3  cannot make real suggest improv wonder section...  \n",
       "4                         sir hero chanc rememb page  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(language = 'english')\n",
    "def stem_list_of_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    stemmed_sent = ' '.join(stemmed_words)  \n",
    "    return stemmed_sent\n",
    "        \n",
    "\n",
    "cleaned_df['stemmed'] = cleaned_df['for_stemming_use'].apply(stem_list_of_words)\n",
    "del cleaned_df['for_stemming_use']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekkah21N6lOz"
   },
   "source": [
    "## Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5xYltEgL6lO0"
   },
   "outputs": [],
   "source": [
    "def to_word_tokens(sent_tokens):\n",
    "    word_tokens = [] \n",
    "    for sent_token in sent_tokens:\n",
    "        word_tokens.append(tt.tokenize(sent_token))\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>for_tagging_use</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>for_tagging_use_sent_token</th>\n",
       "      <th>for_tagging_use_word_token</th>\n",
       "      <th>POS_tagging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "      <td>[explanation\\nwhy the edits made under my user...</td>\n",
       "      <td>[[explanation, why, the, edits, made, under, m...</td>\n",
       "      <td>[[(explanation, NN), (why, WRB), (the, DT), (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>daww match background colour seem stuck thank ...</td>\n",
       "      <td>[d'aww!, he matches this background colour i a...</td>\n",
       "      <td>[[d'aww, !], [he, matches, this, background, c...</td>\n",
       "      <td>[[(d'aww, NN), (!, .)], [(he, PRP), (matches, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>hey man realli tri edit war guy constant remov...</td>\n",
       "      <td>[hey man, i am really not trying to edit war.,...</td>\n",
       "      <td>[[hey, man, ,, i, am, really, not, trying, to,...</td>\n",
       "      <td>[[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni cannot make any real suggestions on...</td>\n",
       "      <td>cannot make real suggestions improvement wonde...</td>\n",
       "      <td>cannot make real suggest improv wonder section...</td>\n",
       "      <td>[\"\\nmore\\ni cannot make any real suggestions o...</td>\n",
       "      <td>[[\", more, i, cannot, make, any, real, suggest...</td>\n",
       "      <td>[[(\", IN), (more, JJR), (i, JJ), (cannot, NNS)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>sir hero chanc rememb page</td>\n",
       "      <td>[you, sir, are my hero., any chance you rememb...</td>\n",
       "      <td>[[you, ,, sir, ,, are, my, hero, .], [any, cha...</td>\n",
       "      <td>[[(you, PRP), (,, ,), (sir, VB), (,, ,), (are,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                    for_tagging_use  \\\n",
       "0              0  explanation\\nwhy the edits made under my usern...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \"\\nmore\\ni cannot make any real suggestions on...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                  text_no_stop_words  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  daww matches background colour seemingly stuck...   \n",
       "2  hey man really trying edit war guy constantly ...   \n",
       "3  cannot make real suggestions improvement wonde...   \n",
       "4                      sir hero chance remember page   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  explan edit made usernam hardcor metallica fan...   \n",
       "1  daww match background colour seem stuck thank ...   \n",
       "2  hey man realli tri edit war guy constant remov...   \n",
       "3  cannot make real suggest improv wonder section...   \n",
       "4                         sir hero chanc rememb page   \n",
       "\n",
       "                          for_tagging_use_sent_token  \\\n",
       "0  [explanation\\nwhy the edits made under my user...   \n",
       "1  [d'aww!, he matches this background colour i a...   \n",
       "2  [hey man, i am really not trying to edit war.,...   \n",
       "3  [\"\\nmore\\ni cannot make any real suggestions o...   \n",
       "4  [you, sir, are my hero., any chance you rememb...   \n",
       "\n",
       "                          for_tagging_use_word_token  \\\n",
       "0  [[explanation, why, the, edits, made, under, m...   \n",
       "1  [[d'aww, !], [he, matches, this, background, c...   \n",
       "2  [[hey, man, ,, i, am, really, not, trying, to,...   \n",
       "3  [[\", more, i, cannot, make, any, real, suggest...   \n",
       "4  [[you, ,, sir, ,, are, my, hero, .], [any, cha...   \n",
       "\n",
       "                                         POS_tagging  \n",
       "0  [[(explanation, NN), (why, WRB), (the, DT), (e...  \n",
       "1  [[(d'aww, NN), (!, .)], [(he, PRP), (matches, ...  \n",
       "2  [[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, ...  \n",
       "3  [[(\", IN), (more, JJR), (i, JJ), (cannot, NNS)...  \n",
       "4  [[(you, PRP), (,, ,), (sir, VB), (,, ,), (are,...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['for_tagging_use_sent_token'] = cleaned_df['for_tagging_use'].apply(sent_tokenize)\n",
    "cleaned_df['for_tagging_use_word_token'] = cleaned_df['for_tagging_use_sent_token'].apply(to_word_tokens)\n",
    "cleaned_df['POS_tagging'] = cleaned_df['for_tagging_use_word_token'].apply(pos_tag_sents)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>POS_tagging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "      <td>[[(explanation, NN), (why, WRB), (the, DT), (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>daww match background colour seem stuck thank ...</td>\n",
       "      <td>[[(d'aww, NN), (!, .)], [(he, PRP), (matches, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>hey man realli tri edit war guy constant remov...</td>\n",
       "      <td>[[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cannot make real suggestions improvement wonde...</td>\n",
       "      <td>cannot make real suggest improv wonder section...</td>\n",
       "      <td>[[(\", IN), (more, JJR), (i, JJ), (cannot, NNS)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>sir hero chanc rememb page</td>\n",
       "      <td>[[(you, PRP), (,, ,), (sir, VB), (,, ,), (are,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                 text_no_stop_words  \\\n",
       "0              0  explanation edits made username hardcore metal...   \n",
       "1              0  daww matches background colour seemingly stuck...   \n",
       "2              0  hey man really trying edit war guy constantly ...   \n",
       "3              0  cannot make real suggestions improvement wonde...   \n",
       "4              0                      sir hero chance remember page   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  explan edit made usernam hardcor metallica fan...   \n",
       "1  daww match background colour seem stuck thank ...   \n",
       "2  hey man realli tri edit war guy constant remov...   \n",
       "3  cannot make real suggest improv wonder section...   \n",
       "4                         sir hero chanc rememb page   \n",
       "\n",
       "                                         POS_tagging  \n",
       "0  [[(explanation, NN), (why, WRB), (the, DT), (e...  \n",
       "1  [[(d'aww, NN), (!, .)], [(he, PRP), (matches, ...  \n",
       "2  [[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, ...  \n",
       "3  [[(\", IN), (more, JJR), (i, JJ), (cannot, NNS)...  \n",
       "4  [[(you, PRP), (,, ,), (sir, VB), (,, ,), (are,...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = cleaned_df.drop(columns=['for_tagging_use_sent_token','for_tagging_use','for_tagging_use_word_token'])\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO6wgTNa6lO0"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:16:10.879978Z",
     "start_time": "2023-03-09T05:16:10.868387Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e65Rfmbr6lO0",
    "outputId": "9641a15f-beb2-48fa-8d06-eedf55673552"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:06:06.788926Z",
     "start_time": "2023-03-09T05:06:05.537963Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A3G-XVR6lO0",
    "outputId": "2e0641c8-3f22-41a1-e655-16ac26179aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now892053827\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/geraldinesoo/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m,cleaned_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m lm \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lm))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/geraldinesoo/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"before\",cleaned_df.iloc[0]['clean_text'])\n",
    "lm = [lemmatizer.lemmatize(t) for t in word_tokenize(cleaned_df.iloc[0]['clean_text'])]\n",
    "print(\"after\",\" \".join(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:38:35.700669Z",
     "start_time": "2023-03-09T05:38:35.616064Z"
    },
    "id": "U-IL5wa96lO0"
   },
   "outputs": [],
   "source": [
    "def process(sentence, stop):\n",
    "    wordsList = nltk.word_tokenize(sentence)\n",
    " \n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop]\n",
    " \n",
    "    #  Using a Tagger. Which is part-of-speech\n",
    "    # tagger or POS-tagger.\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    pos_tagged_text = [ word for word, tag in tagged if tag.startswith('JJ') or tag.startswith('NN') or tag.startswith('RB') or tag.startswith('VB')]\n",
    "    return pos_tagged_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-09T05:39:24.174Z"
    },
    "id": "dXoxUGix6lO0"
   },
   "outputs": [],
   "source": [
    "cleaned_df['lemmatization'] = cleaned_df['clean_text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(t) for t in process(x, stop)]))\n",
    "#cleaned_df['lemmatization_w_pos'] = cleaned_df['taged_words'].apply(lambda x: \" \".join([lemmatizer.lemmatize(t) for t in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:37:14.085902Z",
     "start_time": "2023-03-09T03:37:14.080385Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QD92dGuQ6lO0",
    "outputId": "43fe9e7f-16e2-43c2-81d8-9d9c360fcce0"
   },
   "outputs": [],
   "source": [
    "print(\"before\",cleaned_df.iloc[5]['clean_text'])\n",
    "print(\"after\",cleaned_df.iloc[5]['lemmatization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uba5KcyE6lO1"
   },
   "source": [
    "## Weighted Words - Bag of Words (BoW) - Bag of n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTtIaSNjQ571"
   },
   "source": [
    "### Frequency Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:07:42.198387Z",
     "start_time": "2023-03-09T05:07:42.194562Z"
    },
    "id": "g8ZK10iB6lO1"
   },
   "outputs": [],
   "source": [
    "def bagging(ngram,cleaned_df):\n",
    "  count_vec = CountVectorizer(ngram_range = (ngram, ngram))\n",
    "  weight_words = count_vec.fit_transform(cleaned_df['lemmatization'])\n",
    "  return pd.DataFrame(weight_words.toarray().transpose(), index=count_vec.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T05:07:51.433515Z",
     "start_time": "2023-03-09T05:07:44.080948Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "toH_aJKA6lO1",
    "outputId": "0b8cf0ca-4abb-415f-d044-812566e1fd26"
   },
   "outputs": [],
   "source": [
    "uni_gram = bagging(1, cleaned_df)\n",
    "uni_gram.to_csv(\"/BOW_Unigram.csv\")\n",
    "bi_gram = bagging(2, cleaned_df)\n",
    "bi_gram.to_csv(\"/BOW_Bigram.csv\")\n",
    "tri_gram = bagging(3, cleaned_df)\n",
    "tri_gram.to_csv(\"/BOW_Trigram.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcKLU4DFQ-tA"
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qva3jj-REVB"
   },
   "outputs": [],
   "source": [
    "def TF_IDF(ngram, cleaned_df):\n",
    "    tfidf = TfidfVectorizer(ngram_range = (ngram, ngram))\n",
    "    weight_words = tfidf.fit_transform(cleaned_df['lemmatization'])\n",
    "    return pd.DataFrame(weight_words.toarray().transpose(), index=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "5vSKsKakRn2k",
    "outputId": "fb2fb140-17d5-4ad1-fba1-8c8db9d35e10"
   },
   "outputs": [],
   "source": [
    "uni_gram = TF_IDF(1, cleaned_df)\n",
    "# uni_gram.to_csv(\"/TF_IDF_Unigram.csv\")\n",
    "# bi_gram = TF_IDF(2, cleaned_df)\n",
    "# bi_gram.to_csv(\"/TF_IDF_Bigram.csv\")\n",
    "# tri_gram = TF_IDF(3, cleaned_df)\n",
    "# tri_gram.to_csv(\"/TF_IDF_Trigram.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
