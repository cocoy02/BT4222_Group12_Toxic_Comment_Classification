{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg2_TL3e6lOf"
   },
   "source": [
    "# Table of Content\n",
    "1. [Text Cleaning](#textcleaning)\n",
    "2. [Text Preprocessing](#textpreprocessing)\n",
    "3. [EDA](#EDA)\n",
    "4. [Feature Engineering](#featureengineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T07:45:41.145019Z",
     "start_time": "2023-03-21T07:45:36.917475Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J8voFUP6lOk",
    "outputId": "5045820d-18ed-4e80-caff-e1bb780b7617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# libraries for text cleaning\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "#from profanity_check import predict\n",
    "\n",
    "#for oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# libraries and packages for text (pre-)processing \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "##libraries and packages for EDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T07:45:51.436844Z",
     "start_time": "2023-03-21T07:45:50.417353Z"
    },
    "id": "gBVdsuOL6lOm",
    "outputId": "1ea0c0a0-68f4-45c9-b3b1-bbf67992a36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_pY2Oud6lOm"
   },
   "source": [
    "<a id=\"textcleaning\"></a>\n",
    "# 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBzMaVSL6lOn"
   },
   "source": [
    "## Convert to Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvCX9VN26lOn"
   },
   "source": [
    "We convert all letters to lower case to prepare for the following steps of text cleaning. Exceptional cases such as capital abbreviation will be solved by replacing typos, slang, acronyms or informal abbreviations technique in the subsquent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:25:51.326552Z",
     "start_time": "2023-03-15T01:25:51.101840Z"
    },
    "id": "wtvId9rN6lOo",
    "outputId": "5032f3d0-98e3-45e2-98fd-d116f032911c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  explanation\\nwhy the edits made under my usern...  \n",
       "1  d'aww! he matches this background colour i'm s...  \n",
       "2  hey man, i'm really not trying to edit war. it...  \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...  \n",
       "4  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"] = train_df[\"comment_text\"].apply(lambda x: x.lower())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geAQ_vKu6lOo"
   },
   "source": [
    "## Expand Contractions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjaEJT1X6lOp"
   },
   "source": [
    "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. Removing contractions helps contribute to text standardization. We use contractions package to expand contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:08.373658Z",
     "start_time": "2023-03-15T01:26:04.863712Z"
    },
    "id": "S-5332zZ6lOp"
   },
   "outputs": [],
   "source": [
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:08.546470Z",
     "start_time": "2023-03-15T01:26:08.542617Z"
    },
    "id": "vZuu_RWz6lOq",
    "outputId": "5758de09-ffe5-4a9d-a433-9b3148d2054a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "Clean text: \n",
      " hey man, i am really not trying to edit war. it is just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\n"
     ]
    }
   ],
   "source": [
    "# check if expand contractions works\n",
    "print(\"Original text: \\n\", train_df[\"comment_text\"][2])\n",
    "print(\"Clean text: \\n\", train_df[\"clean_text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDDjj6Yd6lOq"
   },
   "source": [
    "## Remove Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s537aUzz6lOr"
   },
   "source": [
    "Remove unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY2PrjNg6lOr"
   },
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:18.828708Z",
     "start_time": "2023-03-15T01:26:18.161707Z"
    },
    "id": "5dEkw9uT6lOr"
   },
   "outputs": [],
   "source": [
    "# replace URL with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz_zbwDL6lOr"
   },
   "source": [
    "###  Remove Non-ASCI Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:21.601432Z",
     "start_time": "2023-03-15T01:26:20.963405Z"
    },
    "id": "yvhlq6co6lOr"
   },
   "outputs": [],
   "source": [
    "# replace Non_ASCI characters with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(r'[^\\x00-\\x7f]', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TULwYdZ_6lOs"
   },
   "source": [
    "###  Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:23.370112Z",
     "start_time": "2023-03-15T01:26:23.363373Z"
    },
    "id": "qRe8qOFS6lOs"
   },
   "outputs": [],
   "source": [
    "regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        \"]+\", flags = re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:35.728951Z",
     "start_time": "2023-03-15T01:26:34.194567Z"
    },
    "id": "wABsaJiH6lOs"
   },
   "outputs": [],
   "source": [
    "# replace special characters with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: regrex_pattern.sub(' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:26:37.959138Z",
     "start_time": "2023-03-15T01:26:37.954781Z"
    },
    "id": "zNOJbfEO6lOs",
    "outputId": "7eff2ec8-4539-4dea-df02-53aef089518b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " \"P.S. It's not polite to talk to people behind their backs, please remove your comments from Mrph's talk page.\n",
      "\n",
      "Vaughan\n",
      "You're right; I went to check your previous edit and found a page on the Marvel site that spelled it \"\"Vaughn\"\", but now I am finding many more that spell it correctly. Thanks for the edits.   (☎☓) \n",
      "\n",
      "\"\n",
      "Clean text: \n",
      " \"p.s. it is not polite to talk to people behind their backs, please remove your comments from mrph's talk page.\n",
      "\n",
      "vaughan\n",
      "you are right; i went to check your previous edit and found a page on the marvel site that spelled it \"\"vaughn\"\", but now i am finding many more that spell it correctly. thanks for the edits.   (  ) \n",
      "\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# check if special characters are removed\n",
    "print(\"Original text: \\n\", train_df[\"comment_text\"][143])\n",
    "print(\"Clean text: \\n\", train_df[\"clean_text\"][143])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vGfsmkh6lOt"
   },
   "source": [
    "### Remove HTML Tag (BeautifulSoup not really useful? merely remove space?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:27:26.914767Z",
     "start_time": "2023-03-15T01:27:02.048340Z"
    },
    "id": "V0eV01hG6lOt"
   },
   "outputs": [],
   "source": [
    "cleaned_text = train_df[\"clean_text\"].apply(lambda x: BeautifulSoup(str(x)).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:27:27.193504Z",
     "start_time": "2023-03-15T01:27:27.149444Z"
    },
    "id": "mZsyFPAE6lOt"
   },
   "outputs": [],
   "source": [
    "text_changed = cleaned_text!=train_df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:27:27.388076Z",
     "start_time": "2023-03-15T01:27:27.370097Z"
    },
    "id": "dkgNWgJj6lOt",
    "outputId": "3b3c64cf-6db7-4b14-c755-bc0d5692f6b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[569, 1403, 2028, 2189, 3458, 3500, 3687, 4001, 4030, 4090]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i, x in enumerate(text_changed) if x][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:27:27.576620Z",
     "start_time": "2023-03-15T01:27:27.570473Z"
    },
    "id": "d4WcMVMS6lOt",
    "outputId": "fabe66a0-f4d1-47ee-a7f8-b892a15d48e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   heritage from village           in macedonian          . sources claim that the village was pure slavic.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"][228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:27:27.840265Z",
     "start_time": "2023-03-15T01:27:27.836305Z"
    },
    "id": "rin-KW5_6lOt",
    "outputId": "85575126-ad60-4557-8a0a-5d74c72341fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   heritage from village           in macedonian          . sources claim that the village was pure slavic.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:27:28.686819Z",
     "start_time": "2023-03-15T01:27:28.031132Z"
    },
    "id": "QqsIpux16lOt"
   },
   "outputs": [],
   "source": [
    "# replace HTML tag with space\n",
    "html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(html, \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMbjXAwP6lOu"
   },
   "source": [
    "###  Remove Extra Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:28:14.254269Z",
     "start_time": "2023-03-15T01:28:13.975957Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace \\n with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub('\\n', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:28:15.911226Z",
     "start_time": "2023-03-15T01:28:15.746354Z"
    },
    "id": "dOC4PGmJ6lOu"
   },
   "outputs": [],
   "source": [
    "# replace \\r\\n with space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub('\\r\\n', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:28:21.187191Z",
     "start_time": "2023-03-15T01:28:18.220731Z"
    },
    "id": "CLii6lnk6lOu"
   },
   "outputs": [],
   "source": [
    "# remove extra space\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68yB3AkH6lOu"
   },
   "source": [
    "## Replace Common Slangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbEb5Lkj6lOu"
   },
   "source": [
    "Slang, acronyms or informal abbreviations should be replaced with formal English. The list of common slangs used in Tweets takes reference from https://www.kaggle.com/code/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:28:24.182681Z",
     "start_time": "2023-03-15T01:28:24.171969Z"
    },
    "id": "IeiaTG9q6lOu",
    "outputId": "65762560-59ce-4895-e0c4-c63595c36c8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$</td>\n",
       "      <td>dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>€</td>\n",
       "      <td>euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ao</td>\n",
       "      <td>for adults only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a.m</td>\n",
       "      <td>before midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3</td>\n",
       "      <td>anytime anywhere anyplace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abbreviation                translation\n",
       "0            $                    dollar \n",
       "1            €                      euro \n",
       "2          4ao            for adults only\n",
       "3          a.m              before midday\n",
       "4           a3  anytime anywhere anyplace"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read abbreviation.csv\n",
    "abbreviations = pd.read_csv('Data/abbreviations.csv')\n",
    "abbreviations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:28:25.761324Z",
     "start_time": "2023-03-15T01:28:25.757622Z"
    },
    "id": "tDR-y7Bw6lOv"
   },
   "outputs": [],
   "source": [
    "# convert the data frame to a dictionary\n",
    "abbreviations_dict = dict(zip(abbreviations.abbreviation, abbreviations.translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:28:32.216476Z",
     "start_time": "2023-03-15T01:28:32.212116Z"
    },
    "id": "_TAqz0A16lOv"
   },
   "outputs": [],
   "source": [
    "# define a helper function to replace the abbreviations\n",
    "def convert_abbrev(text):\n",
    "    # create a pattern of all abbreviations and make sure they are not part of a longer word\n",
    "    abbreviations_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in abbreviations_dict.keys()) + r')(?!\\w)')\n",
    "    # replace an abbreviation with its translation\n",
    "    text = abbreviations_pattern.sub(lambda x: abbreviations_dict[x.group()], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:29:02.918480Z",
     "start_time": "2023-03-15T01:28:35.208812Z"
    },
    "id": "vt7rxipN6lOv"
   },
   "outputs": [],
   "source": [
    "# replace the slangs\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(convert_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:29:03.135434Z",
     "start_time": "2023-03-15T01:29:03.132156Z"
    },
    "id": "Ap-nXTcz6lOv",
    "outputId": "f2e4517b-f415-4a02-b0fc-5aec91b38368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "Clean text: \n",
      " d'aww! he matches this background colour i am seemingly stuck with. thanks. (talk) 21:51, january 11, 2016 (coordinated universal time)\n"
     ]
    }
   ],
   "source": [
    "# check if slangs are replaced\n",
    "print(\"Original text: \\n\", train_df[\"comment_text\"][1])\n",
    "print(\"Clean text: \\n\", train_df[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emHZdEXQ6lOv"
   },
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2STK8pdp6lOv"
   },
   "source": [
    "We should correct the misspellings in the text. Both SpellChecker and TextBlob provide such functions, and we would like to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:29:28.104592Z",
     "start_time": "2023-03-15T01:29:28.094770Z"
    },
    "id": "IZqlFmW-6lOw"
   },
   "outputs": [],
   "source": [
    "# select random texts from clean_text\n",
    "length = len(train_df[\"clean_text\"])\n",
    "random_num = random.sample(range(length), 100)\n",
    "random_text = train_df[\"clean_text\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:30:42.627950Z",
     "start_time": "2023-03-15T01:29:29.954546Z"
    },
    "id": "BQIDnE316lOw",
    "outputId": "bc74cb63-5169-4496-882f-2050b46a2ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 30.84517478942871 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# using TextBlob package\n",
    "start_time1 = time.time()\n",
    "random_text.apply(lambda x: TextBlob(x).correct())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:30:57.379143Z",
     "start_time": "2023-03-15T01:30:42.839902Z"
    },
    "id": "WkNiuhEN6lOw",
    "outputId": "b5d9ab68-5f20-46f4-f43f-2cb89d61aa40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 16.331270933151245 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# using SpellChecker package\n",
    "start_time2 = time.time()\n",
    "random_text.apply(lambda x: SpellChecker().correction(x))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq8ZyOhp6lOw"
   },
   "source": [
    "Randomly select 100 texts and apply spelling correction functions on them. Comparing the execution time of 2 different packages, SpellChecker is much faster than TextBlob. Considering we are using a large-scale dataset, SpellChecker is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:30:57.595335Z",
     "start_time": "2023-03-15T01:30:57.590891Z"
    },
    "id": "z86lgIkR6lOw"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    start_time = time.time()\n",
    "    cleaned_text = []\n",
    "    spellchecker = SpellChecker()\n",
    "    for i in range(text.shape[0]):\n",
    "        if i%100==0:\n",
    "            print(f'{i}-th text is being processed')\n",
    "        cleaned_text.append(spellchecker.correction(text[i]))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:32:28.939270Z",
     "start_time": "2023-03-15T01:30:57.798630Z"
    },
    "id": "VG5votHY6lOx",
    "outputId": "90e75b6c-ae9b-4f09-ae26-84b028ac1f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th text is being processed\n",
      "100-th text is being processed\n",
      "200-th text is being processed\n",
      "300-th text is being processed\n",
      "400-th text is being processed\n",
      "500-th text is being processed\n",
      "600-th text is being processed\n",
      "700-th text is being processed\n",
      "800-th text is being processed\n",
      "900-th text is being processed\n",
      "1000-th text is being processed\n",
      "--- 67.02877187728882 seconds ---\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = correct_spelling(train_df[\"clean_text\"][:1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:32:29.172738Z",
     "start_time": "2023-03-15T01:32:29.168067Z"
    },
    "id": "1EHbF5xg6lOx",
    "outputId": "eea78c40-2a6a-4195-cae3-f74430fbd843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 62,  89, 101, 173, 175, 211, 217, 223, 226, 241, 250, 254, 259,\n",
       "            268, 276, 299, 320, 323, 376, 381, 397, 408, 423, 448, 465, 470,\n",
       "            504, 545, 592, 627, 632, 646, 715, 743, 758, 787, 806, 807, 814,\n",
       "            823, 831, 844, 852, 874, 877, 883, 897, 899, 913, 923, 947, 971],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"][:1001].index[cleaned_text!=train_df[\"clean_text\"][:1001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:32:29.399573Z",
     "start_time": "2023-03-15T01:32:29.395943Z"
    },
    "id": "fxxHMgpT6lOx",
    "outputId": "c11d4091-10dd-4924-aade-edbb2b114cf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. fu ck ing trollreasons'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"clean_text\"][971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:32:43.104301Z",
     "start_time": "2023-03-15T01:32:43.101099Z"
    },
    "id": "3Cv-gPsz6lOx"
   },
   "outputs": [],
   "source": [
    "cleaned_text[971]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_rHvPza6lOx"
   },
   "source": [
    "However, many corrections do not make sense, and may omit some useful information. We decided not to use established package to perform spelling correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOQbOAg56lOx"
   },
   "source": [
    "## Remove Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DIYDhBP6lOx"
   },
   "source": [
    "We remove punctuations from the text as the final step of text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:04:55.594368Z",
     "start_time": "2023-03-15T02:04:53.563284Z"
    },
    "id": "5IDueq1s6lOy"
   },
   "outputs": [],
   "source": [
    "# # remove punctuations\n",
    "#train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "r = re.compile(r'''([!#$%&'()*+,./:;<=>?@[\\]^_`{|}~-])[!\"#$%&'()*+,./:;<=>?@[\\]^_`{|}~-]+''')\n",
    "train_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda x: r.sub(r'\\1', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:04:57.702546Z",
     "start_time": "2023-03-15T02:04:57.684242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\":and for the second time of asking, when your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm, there is no actual article for pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" and . i really do not think you understand. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "159566      0             0        0       0       0              0   \n",
       "159567      0             0        0       0       0              0   \n",
       "159568      0             0        0       0       0              0   \n",
       "159569      0             0        0       0       0              0   \n",
       "159570      0             0        0       0       0              0   \n",
       "\n",
       "                                               clean_text  \n",
       "0       explanation why the edits made under my userna...  \n",
       "1       d'aww! he matches this background colour i am ...  \n",
       "2       hey man, i am really not trying to edit war. i...  \n",
       "3       \" more i cannot make any real suggestions on i...  \n",
       "4       you, sir, are my hero. any chance you remember...  \n",
       "...                                                   ...  \n",
       "159566  \":and for the second time of asking, when your...  \n",
       "159567  you should be ashamed of yourself that is a ho...  \n",
       "159568  spitzer umm, there is no actual article for pr...  \n",
       "159569  and it looks like it was actually you who put ...  \n",
       "159570  \" and . i really do not think you understand. ...  \n",
       "\n",
       "[159571 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:05:29.561246Z",
     "start_time": "2023-03-15T02:05:26.651119Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.drop('comment_text', axis=1).to_csv('Data/cleaned_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP1_2vo36lOy"
   },
   "source": [
    "<a id=\"textpreprocessing\"></a>\n",
    "# 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:26:04.941821Z",
     "start_time": "2023-03-15T02:26:03.711748Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WeV9pWgO6lOy",
    "outputId": "a4bcd494-6401-492a-eda6-bf797c534e76",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \n",
       "0              0  explanation why the edits made under my userna...  \n",
       "1              0  d'aww! he matches this background colour i am ...  \n",
       "2              0  hey man, i am really not trying to edit war. i...  \n",
       "3              0  \" more i cannot make any real suggestions on i...  \n",
       "4              0  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned_df = pd.read_csv(\"/cleaned_train.csv\")\n",
    "cleaned_df = pd.read_csv(\"Data/cleaned_train.csv\")\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnWPmf9h6lOz"
   },
   "source": [
    "## Tokenization (NLTK has a package specially catered to tokenizing tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:27:05.516093Z",
     "start_time": "2023-03-15T02:26:05.762781Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "cleaned_df['text_tokenized'] = cleaned_df['clean_text'].apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:27:05.977056Z",
     "start_time": "2023-03-15T02:27:05.964852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[d'aww, !, he, matches, this, background, colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, ,, i, am, really, not, trying, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "      <td>[\", more, i, cannot, make, any, real, suggesti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, ,, sir, ,, are, my, hero, ., any, chance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation why the edits made under my userna...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \" more i cannot make any real suggestions on i...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d'aww, !, he, matches, this, background, colo...  \n",
       "2  [hey, man, ,, i, am, really, not, trying, to, ...  \n",
       "3  [\", more, i, cannot, make, any, real, suggesti...  \n",
       "4  [you, ,, sir, ,, are, my, hero, ., any, chance...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi9Zr5yH6lOz"
   },
   "source": [
    "## Remove Stop Words (or/and Frequent words/ Rare words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:27:06.348584Z",
     "start_time": "2023-03-15T02:27:06.340783Z"
    },
    "id": "J2zjB4hb6lOz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#review all stop words in the library\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:28:17.186718Z",
     "start_time": "2023-03-15T02:27:06.671121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[d'aww, !, he, matches, this, background, colo...</td>\n",
       "      <td>d'aww ! matches background colour seemingly st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, ,, i, am, really, not, trying, to, ...</td>\n",
       "      <td>hey man , really trying edit war . guy constan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "      <td>[\", more, i, cannot, make, any, real, suggesti...</td>\n",
       "      <td>\" cannot make real suggestions improvement - w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, ,, sir, ,, are, my, hero, ., any, chance...</td>\n",
       "      <td>, sir , hero . chance remember page ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation why the edits made under my userna...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \" more i cannot make any real suggestions on i...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww, !, he, matches, this, background, colo...   \n",
       "2  [hey, man, ,, i, am, really, not, trying, to, ...   \n",
       "3  [\", more, i, cannot, make, any, real, suggesti...   \n",
       "4  [you, ,, sir, ,, are, my, hero, ., any, chance...   \n",
       "\n",
       "                                  text_no_stop_words  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  d'aww ! matches background colour seemingly st...  \n",
       "2  hey man , really trying edit war . guy constan...  \n",
       "3  \" cannot make real suggestions improvement - w...  \n",
       "4              , sir , hero . chance remember page ?  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words from tokenized text\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    # check in lowercase \n",
    "    t = [token for token in row['text_tokenized'] if token.lower() not in stop_words]\n",
    "    text = ' '.join(t)    \n",
    "    return pd.Series([text])    #for stemming\n",
    "\n",
    "cleaned_df[['text_no_stop_words']] = cleaned_df.apply(remove_stopwords, axis = 1)\n",
    "\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:16:18.847481Z",
     "start_time": "2023-03-15T02:16:18.824175Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[d'aww, !, he, matches, this, background, colo...</td>\n",
       "      <td>d'aww ! matches background colour seemingly st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, ,, i, am, really, not, trying, to, ...</td>\n",
       "      <td>hey man , really trying edit war . guy constan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "      <td>[\", more, i, cannot, make, any, real, suggesti...</td>\n",
       "      <td>\" cannot make real suggestions improvement - w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, ,, sir, ,, are, my, hero, ., any, chance...</td>\n",
       "      <td>, sir , hero . chance remember page ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation why the edits made under my userna...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \" more i cannot make any real suggestions on i...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww, !, he, matches, this, background, colo...   \n",
       "2  [hey, man, ,, i, am, really, not, trying, to, ...   \n",
       "3  [\", more, i, cannot, make, any, real, suggesti...   \n",
       "4  [you, ,, sir, ,, are, my, hero, ., any, chance...   \n",
       "\n",
       "                                  text_no_stop_words  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  d'aww ! matches background colour seemingly st...  \n",
       "2  hey man , really trying edit war . guy constan...  \n",
       "3  \" cannot make real suggestions improvement - w...  \n",
       "4              , sir , hero . chance remember page ?  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop clean_text and tokenized column\n",
    "#del cleaned_df['clean_text']\n",
    "#del cleaned_df['text_tokenized']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7E0_XHB6lOz"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br2k1Vb_6lOz"
   },
   "source": [
    "We will use Snowball Stemmer to realise stemming. Comparing with Poster Stemmer, it is more efficient and has higher performance. Comparing with Lancaster Stemmer, it is less aggressive and can keep more word meanings for the Semantic Analysis in our later stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:29:50.193076Z",
     "start_time": "2023-03-15T02:28:17.620473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explan whi the edit made under my usernam hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[d'aww, !, he, matches, this, background, colo...</td>\n",
       "      <td>d'aww ! matches background colour seemingly st...</td>\n",
       "      <td>d'aww ! he match this background colour i am s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, ,, i, am, really, not, trying, to, ...</td>\n",
       "      <td>hey man , really trying edit war . guy constan...</td>\n",
       "      <td>hey man , i am realli not tri to edit war . it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "      <td>[\", more, i, cannot, make, any, real, suggesti...</td>\n",
       "      <td>\" cannot make real suggestions improvement - w...</td>\n",
       "      <td>\" more i cannot make ani real suggest on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, ,, sir, ,, are, my, hero, ., any, chance...</td>\n",
       "      <td>, sir , hero . chance remember page ?</td>\n",
       "      <td>you , sir , are my hero . ani chanc you rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation why the edits made under my userna...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \" more i cannot make any real suggestions on i...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww, !, he, matches, this, background, colo...   \n",
       "2  [hey, man, ,, i, am, really, not, trying, to, ...   \n",
       "3  [\", more, i, cannot, make, any, real, suggesti...   \n",
       "4  [you, ,, sir, ,, are, my, hero, ., any, chance...   \n",
       "\n",
       "                                  text_no_stop_words  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  d'aww ! matches background colour seemingly st...   \n",
       "2  hey man , really trying edit war . guy constan...   \n",
       "3  \" cannot make real suggestions improvement - w...   \n",
       "4              , sir , hero . chance remember page ?   \n",
       "\n",
       "                                             stemmed  \n",
       "0  explan whi the edit made under my usernam hard...  \n",
       "1  d'aww ! he match this background colour i am s...  \n",
       "2  hey man , i am realli not tri to edit war . it...  \n",
       "3  \" more i cannot make ani real suggest on impro...  \n",
       "4  you , sir , are my hero . ani chanc you rememb...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(language = 'english')\n",
    "def stem_list_of_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    stemmed_sent = ' '.join(stemmed_words)  \n",
    "    return stemmed_sent\n",
    "        \n",
    "\n",
    "cleaned_df['stemmed'] = cleaned_df['text_tokenized'].apply(stem_list_of_words)\n",
    "#del cleaned_df['for_stemming_use']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekkah21N6lOz"
   },
   "source": [
    "## Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:30:33.828701Z",
     "start_time": "2023-03-15T02:30:33.819975Z"
    },
    "id": "5xYltEgL6lO0"
   },
   "outputs": [],
   "source": [
    "def to_word_tokens(sent_tokens):\n",
    "    word_tokens = [] \n",
    "    for sent_token in sent_tokens:\n",
    "        word_tokens.append(tt.tokenize(sent_token))\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:39:58.588745Z",
     "start_time": "2023-03-15T02:30:37.465488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>for_tagging_use_sent_token</th>\n",
       "      <th>for_tagging_use_word_token</th>\n",
       "      <th>POS_tagging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explan whi the edit made under my usernam hard...</td>\n",
       "      <td>[explanation why the edits made under my usern...</td>\n",
       "      <td>[[explanation, why, the, edits, made, under, m...</td>\n",
       "      <td>[[(explanation, NN), (why, WRB), (the, DT), (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i am ...</td>\n",
       "      <td>[d'aww, !, he, matches, this, background, colo...</td>\n",
       "      <td>d'aww ! matches background colour seemingly st...</td>\n",
       "      <td>d'aww ! he match this background colour i am s...</td>\n",
       "      <td>[d'aww!, he matches this background colour i a...</td>\n",
       "      <td>[[d'aww, !], [he, matches, this, background, c...</td>\n",
       "      <td>[[(d'aww, NN), (!, .)], [(he, PRP), (matches, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i am really not trying to edit war. i...</td>\n",
       "      <td>[hey, man, ,, i, am, really, not, trying, to, ...</td>\n",
       "      <td>hey man , really trying edit war . guy constan...</td>\n",
       "      <td>hey man , i am realli not tri to edit war . it...</td>\n",
       "      <td>[hey man, i am really not trying to edit war.,...</td>\n",
       "      <td>[[hey, man, ,, i, am, really, not, trying, to,...</td>\n",
       "      <td>[[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i cannot make any real suggestions on i...</td>\n",
       "      <td>[\", more, i, cannot, make, any, real, suggesti...</td>\n",
       "      <td>\" cannot make real suggestions improvement - w...</td>\n",
       "      <td>\" more i cannot make ani real suggest on impro...</td>\n",
       "      <td>[\" more i cannot make any real suggestions on ...</td>\n",
       "      <td>[[\", more, i, cannot, make, any, real, suggest...</td>\n",
       "      <td>[[(\", IN), (more, JJR), (i, JJ), (cannot, NNS)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you, ,, sir, ,, are, my, hero, ., any, chance...</td>\n",
       "      <td>, sir , hero . chance remember page ?</td>\n",
       "      <td>you , sir , are my hero . ani chanc you rememb...</td>\n",
       "      <td>[you, sir, are my hero., any chance you rememb...</td>\n",
       "      <td>[[you, ,, sir, ,, are, my, hero, .], [any, cha...</td>\n",
       "      <td>[[(you, PRP), (,, ,), (sir, VB), (,, ,), (are,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                         clean_text  \\\n",
       "0              0  explanation why the edits made under my userna...   \n",
       "1              0  d'aww! he matches this background colour i am ...   \n",
       "2              0  hey man, i am really not trying to edit war. i...   \n",
       "3              0  \" more i cannot make any real suggestions on i...   \n",
       "4              0  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww, !, he, matches, this, background, colo...   \n",
       "2  [hey, man, ,, i, am, really, not, trying, to, ...   \n",
       "3  [\", more, i, cannot, make, any, real, suggesti...   \n",
       "4  [you, ,, sir, ,, are, my, hero, ., any, chance...   \n",
       "\n",
       "                                  text_no_stop_words  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  d'aww ! matches background colour seemingly st...   \n",
       "2  hey man , really trying edit war . guy constan...   \n",
       "3  \" cannot make real suggestions improvement - w...   \n",
       "4              , sir , hero . chance remember page ?   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  explan whi the edit made under my usernam hard...   \n",
       "1  d'aww ! he match this background colour i am s...   \n",
       "2  hey man , i am realli not tri to edit war . it...   \n",
       "3  \" more i cannot make ani real suggest on impro...   \n",
       "4  you , sir , are my hero . ani chanc you rememb...   \n",
       "\n",
       "                          for_tagging_use_sent_token  \\\n",
       "0  [explanation why the edits made under my usern...   \n",
       "1  [d'aww!, he matches this background colour i a...   \n",
       "2  [hey man, i am really not trying to edit war.,...   \n",
       "3  [\" more i cannot make any real suggestions on ...   \n",
       "4  [you, sir, are my hero., any chance you rememb...   \n",
       "\n",
       "                          for_tagging_use_word_token  \\\n",
       "0  [[explanation, why, the, edits, made, under, m...   \n",
       "1  [[d'aww, !], [he, matches, this, background, c...   \n",
       "2  [[hey, man, ,, i, am, really, not, trying, to,...   \n",
       "3  [[\", more, i, cannot, make, any, real, suggest...   \n",
       "4  [[you, ,, sir, ,, are, my, hero, .], [any, cha...   \n",
       "\n",
       "                                         POS_tagging  \n",
       "0  [[(explanation, NN), (why, WRB), (the, DT), (e...  \n",
       "1  [[(d'aww, NN), (!, .)], [(he, PRP), (matches, ...  \n",
       "2  [[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, ...  \n",
       "3  [[(\", IN), (more, JJR), (i, JJ), (cannot, NNS)...  \n",
       "4  [[(you, PRP), (,, ,), (sir, VB), (,, ,), (are,...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['for_tagging_use_sent_token'] = cleaned_df['clean_text'].apply(sent_tokenize)\n",
    "cleaned_df['for_tagging_use_word_token'] = cleaned_df['for_tagging_use_sent_token'].apply(to_word_tokens)\n",
    "cleaned_df['POS_tagging'] = cleaned_df['for_tagging_use_word_token'].apply(pos_tag_sents)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:40:29.637451Z",
     "start_time": "2023-03-15T02:40:24.863216Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.drop(columns=['for_tagging_use_sent_token','for_tagging_use_word_token'])\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO6wgTNa6lO0"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:45:20.578303Z",
     "start_time": "2023-03-15T02:45:15.636379Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df['POS_tagging_flat'] = cleaned_df['POS_tagging'].apply(lambda x: [element for innerList in x for element in innerList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T03:17:23.191088Z",
     "start_time": "2023-03-15T03:16:46.816337Z"
    },
    "id": "dXoxUGix6lO0"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_df['lemmatization'] = cleaned_df['POS_tagging_flat'].apply(lambda x: [lemmatizer.lemmatize(word) for word, tag in x \n",
    "                                                                              if ((tag.startswith('JJ') or tag.startswith('NN') or tag.startswith('RB') or tag.startswith('VB')) and (word not in string.punctuation))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming might not be useful for creating features. Lemmatization based on tagging result has achieved same result as text without stop words. Text_tokenized column, POS tagging result, and lemmatization for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T03:04:17.204271Z",
     "start_time": "2023-03-15T03:04:17.105571Z"
    }
   },
   "outputs": [],
   "source": [
    "del cleaned_df['clean_text']\n",
    "del cleaned_df['text_no_stop_words']\n",
    "del cleaned_df['stemmed']\n",
    "del cleaned_df['POS_tagging']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T03:20:09.263987Z",
     "start_time": "2023-03-15T03:19:43.025834Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('Data/processed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"EDA\"></a>\n",
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T07:49:03.282262Z",
     "start_time": "2023-03-21T07:46:15.494890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>POS_tagging_flat</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[(explanation, NN), (why, WRB), (the, DT), (ed...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, !, he, matches, this, background, colo...</td>\n",
       "      <td>[(d'aww, NN), (!, .), (he, PRP), (matches, VBZ...</td>\n",
       "      <td>[d'aww, match, colour, i, am, seemingly, stuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, ,, i, am, really, not, trying, to, ...</td>\n",
       "      <td>[(hey, NN), (man, NN), (,, ,), (i, JJ), (am, V...</td>\n",
       "      <td>[hey, man, i, am, really, not, trying, edit, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[\", more, i, cannot, make, any, real, suggesti...</td>\n",
       "      <td>[(\", IN), (more, JJR), (i, JJ), (cannot, NNS),...</td>\n",
       "      <td>[more, i, cannot, make, real, suggestion, impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, ,, sir, ,, are, my, hero, ., any, chance...</td>\n",
       "      <td>[(you, PRP), (,, ,), (sir, VB), (,, ,), (are, ...</td>\n",
       "      <td>[sir, are, hero, chance, remember, page, is]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0000997932d777bf      0             0        0       0       0   \n",
       "1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2  000113f07ec002fd      0             0        0       0       0   \n",
       "3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                     text_tokenized  \\\n",
       "0              0  [explanation, why, the, edits, made, under, my...   \n",
       "1              0  [d'aww, !, he, matches, this, background, colo...   \n",
       "2              0  [hey, man, ,, i, am, really, not, trying, to, ...   \n",
       "3              0  [\", more, i, cannot, make, any, real, suggesti...   \n",
       "4              0  [you, ,, sir, ,, are, my, hero, ., any, chance...   \n",
       "\n",
       "                                    POS_tagging_flat  \\\n",
       "0  [(explanation, NN), (why, WRB), (the, DT), (ed...   \n",
       "1  [(d'aww, NN), (!, .), (he, PRP), (matches, VBZ...   \n",
       "2  [(hey, NN), (man, NN), (,, ,), (i, JJ), (am, V...   \n",
       "3  [(\", IN), (more, JJR), (i, JJ), (cannot, NNS),...   \n",
       "4  [(you, PRP), (,, ,), (sir, VB), (,, ,), (are, ...   \n",
       "\n",
       "                                       lemmatization  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [d'aww, match, colour, i, am, seemingly, stuck...  \n",
       "2  [hey, man, i, am, really, not, trying, edit, w...  \n",
       "3  [more, i, cannot, make, real, suggestion, impr...  \n",
       "4       [sir, are, hero, chance, remember, page, is]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevent lists from being converted to strings\n",
    "processed_df = pd.read_csv('Data/processed_train.csv', \n",
    "                           converters={'text_tokenized': literal_eval, 'POS_tagging_flat': literal_eval, 'lemmatization': literal_eval})\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_ = [\"deep blue\",\"blue\", \"teal\", \"lightblue\",\"darkblue\",\"purple\"]\n",
    "\n",
    "palette= sns.xkcd_palette(colors_)\n",
    "\n",
    "x = processed_df.iloc[:,1:7].sum()\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "ax= sns.barplot(x=x.index,y=x.values,palette=palette)\n",
    "plt.title(\"Comment Category Distribution in Training set\")\n",
    "plt.xlabel('Label ')\n",
    "plt.ylabel('No. of tweets', fontsize=11)\n",
    "\n",
    "patch = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(patch, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, va='bottom',ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain counts of each class\n",
    "counts = {\n",
    "    '0': [],\n",
    "    '1': [],\n",
    "}\n",
    "\n",
    "categories = list(processed_df.columns.values)\n",
    "categories = categories[1:7]\n",
    "\n",
    "for c in categories:\n",
    "    count_occurence_1 = len(processed_df.loc[processed_df[c] == 1])\n",
    "    count_occurence_0 = len(processed_df.loc[processed_df[c] == 0])\n",
    "    counts['0'].append(count_occurence_0)\n",
    "    counts['1'].append(count_occurence_1)\n",
    "\n",
    "# Plot distribution of comment category\n",
    "x = np.arange(len(categories))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "for attribute, measurement in counts.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Number of Tweets')\n",
    "ax.set_xlabel('Comment Category')\n",
    "ax.set_title('Distribution of Comment Category')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments_count= len(processed_df[processed_df[categories].sum(axis=1) > 0])\n",
    "nontoxic_comments_count = len(processed_df[processed_df[categories].sum(axis=1) == 0])\n",
    "\n",
    "print(\"Total number of comments = \",len(processed_df))\n",
    "print(\"Number of non-toxic comments = \",nontoxic_comments_count)\n",
    "print(\"Number of comments with toxic labels =\",(len(processed_df)-nontoxic_comments_count))\n",
    "\n",
    "print(f\"{round(nontoxic_comments_count/len(processed_df) * 100,3)} % percentage of rows contains only zeros in training data\")\n",
    "\n",
    "count = pd.DataFrame(dict(\n",
    "    types = ['toxic', 'non-toxic'],\n",
    "    counts = [toxic_comments_count, nontoxic_comments_count]\n",
    "))\n",
    "\n",
    "sns.barplot(data=count, x='types',y='counts').set(title='Toxic vs Non-Toxic Tweets Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we also observe that the dataset is highly imbalanced. This fact is also supported by the the big number of difference in toxic and non-toxic comment (about 90% of training data is non-toxic). Therefore, in the later part of the project, we will perform oversampling on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "toxic_comments = processed_df[processed_df[categories].sum(axis=1) > 0]\n",
    "nontoxic_comments = processed_df[processed_df[categories].sum(axis=1) == 0]\n",
    "\n",
    "toxic_comments['class'] = 1 #1 for toxic\n",
    "nontoxic_comments['class'] = 0\n",
    "\n",
    "processed_df_new = pd.concat([toxic_comments, nontoxic_comments], ignore_index=True, axis=0)\n",
    "\n",
    "X_train = processed_df_new.drop('class', axis = 1)\n",
    "y_train = processed_df_new['class']\n",
    "X_train_ros, y_train_ros= ros.fit_resample(X_train, y_train)\n",
    "                              \n",
    "# Check the number of records after over sampling\n",
    "print(X_train_ros.shape)\n",
    "print(sorted(Counter(y_train_ros).items()))\n",
    "                              \n",
    "#Concatenate X_train and y_train\n",
    "train_oversampled = pd.concat([X_train_ros,  y_train_ros], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toxic_comments = train_oversampled[train_oversampled['class']== 1]\n",
    "train_clean_comments = train_oversampled[train_oversampled['class']== 0]\n",
    "\n",
    "count = pd.DataFrame(dict(\n",
    "    types = ['toxic', 'non-toxic'],\n",
    "    counts = [len(train_toxic_comments), len(train_clean_comments)]\n",
    "))\n",
    "\n",
    "sns.barplot(data=count, x='types',y='counts').set(title='Toxic vs Non-Toxic Tweets Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oversampled.to_csv('Data/train_oversampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oversampled['words length'] = train_oversampled['text_tokenized'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train_oversampled, x='words length', title='Word Length Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oversampled['characters length'] = train_oversampled['text_tokenized'].apply(lambda x: ''.join(word for word in x)).apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train_oversampled, x='words length', title='Characters Length Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top frequent tags (using POS result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>POS_tagging_flat</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[cocksucker, before, you, piss, around, on, my...</td>\n",
       "      <td>[(cocksucker, NN), (before, IN), (you, PRP), (...</td>\n",
       "      <td>[cocksucker, piss, around, work]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, ., what, is, it, ., @, |, talk, ., what,...</td>\n",
       "      <td>[(hey, NN), (., .), (what, WP), (is, VBZ), (it...</td>\n",
       "      <td>[hey, is, talk, is, exclusive, group, wp, tali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[bye, !, do, not, look, ,, come, or, think, of...</td>\n",
       "      <td>[(bye, NN), (!, .), (do, VB), (not, RB), (look...</td>\n",
       "      <td>[bye, do, not, look, come, think, comming, bac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[you, are, gay, or, antisemmitian, ?, archange...</td>\n",
       "      <td>[(you, PRP), (are, VBP), (gay, JJ), (or, CC), ...</td>\n",
       "      <td>[are, gay, antisemmitian, archangel, white, ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[fuck, your, filthy, mother, in, the, ass, ,, ...</td>\n",
       "      <td>[(fuck, VB), (your, PRP$), (filthy, JJ), (moth...</td>\n",
       "      <td>[fuck, filthy, mother, as, dry]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  0002bcb3da6cb337      1             1        1       0       1   \n",
       "1  0005c987bdfc9d4b      1             0        0       0       0   \n",
       "2  0007e25b2121310b      1             0        0       0       0   \n",
       "3  001810bf8c45bf5f      1             0        1       0       1   \n",
       "4  00190820581d90ce      1             0        1       0       1   \n",
       "\n",
       "   identity_hate                                     text_tokenized  \\\n",
       "0              0  [cocksucker, before, you, piss, around, on, my...   \n",
       "1              0  [hey, ., what, is, it, ., @, |, talk, ., what,...   \n",
       "2              0  [bye, !, do, not, look, ,, come, or, think, of...   \n",
       "3              1  [you, are, gay, or, antisemmitian, ?, archange...   \n",
       "4              0  [fuck, your, filthy, mother, in, the, ass, ,, ...   \n",
       "\n",
       "                                    POS_tagging_flat  \\\n",
       "0  [(cocksucker, NN), (before, IN), (you, PRP), (...   \n",
       "1  [(hey, NN), (., .), (what, WP), (is, VBZ), (it...   \n",
       "2  [(bye, NN), (!, .), (do, VB), (not, RB), (look...   \n",
       "3  [(you, PRP), (are, VBP), (gay, JJ), (or, CC), ...   \n",
       "4  [(fuck, VB), (your, PRP$), (filthy, JJ), (moth...   \n",
       "\n",
       "                                       lemmatization  class  \n",
       "0                   [cocksucker, piss, around, work]      1  \n",
       "1  [hey, is, talk, is, exclusive, group, wp, tali...      1  \n",
       "2  [bye, do, not, look, come, think, comming, bac...      1  \n",
       "3  [are, gay, antisemmitian, archangel, white, ti...      1  \n",
       "4                    [fuck, filthy, mother, as, dry]      1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df= pd.read_csv(\"Data/train_oversampled.csv\",\n",
    "                           converters={'text_tokenized': literal_eval, 'POS_tagging_flat': literal_eval, 'lemmatization': literal_eval})\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_list = []\n",
    "\n",
    "for x in processed_df['POS_tagging_flat']:\n",
    "    POS_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "POS_count = Counter(chain(*POS_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_frequent_df = pd.DataFrame.from_dict(POS_count, orient='index').reset_index()\n",
    "POS_frequent_df.columns = ['POS_tags', 'Count']\n",
    "POS_frequent_df = POS_frequent_df.sort_values(by=['Count'], ascending=False)\n",
    "POS_frequent_df['Word'] = POS_frequent_df['POS_tags'].apply(lambda x: x[0])\n",
    "POS_frequent_df['Tag'] = POS_frequent_df['POS_tags'].apply(lambda x: x[1])\n",
    "POS_frequent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_punctuations = []\n",
    "punctuations = string.punctuation \n",
    "for x in punctuations:\n",
    "    list_punctuations.append(x)\n",
    "    \n",
    "POS_frequent_df = POS_frequent_df.query(\"Word not in @stop_words & Word not in @list_punctuations\")\n",
    "POS_frequent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Top frequent tags\", fontsize=12)\n",
    "sns.barplot(x='Count', y='Word', hue='Tag', \n",
    "            data=POS_frequent_df.iloc[:20,:], dodge=False, ax=ax)\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic Comments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_POS_list = []\n",
    "\n",
    "for x in processed_df['POS_tagging_flat'][((processed_df.toxic == 1) | (processed_df.severe_toxic == 1) | (processed_df.obscene == 1) | (processed_df.threat == 1) | (processed_df.identity_hate == 1))]:\n",
    "    toxic_POS_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_POS_count = Counter(chain(*toxic_POS_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_POS_frequent_df = pd.DataFrame.from_dict(toxic_POS_count, orient='index').reset_index()\n",
    "toxic_POS_frequent_df.columns = ['toxic_POS_tags', 'Count']\n",
    "toxic_POS_frequent_df = toxic_POS_frequent_df.sort_values(by=['Count'], ascending=False)\n",
    "toxic_POS_frequent_df['Word'] = toxic_POS_frequent_df['toxic_POS_tags'].apply(lambda x: x[0])\n",
    "toxic_POS_frequent_df['Tag'] = toxic_POS_frequent_df['toxic_POS_tags'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_POS_frequent_df = toxic_POS_frequent_df.query(\"Word not in @stop_words & Word not in @list_punctuations\")\n",
    "toxic_POS_frequent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Top frequent toxic tags\", fontsize=12)\n",
    "sns.barplot(x='Count', y='Word', hue='Tag', \n",
    "            data=toxic_POS_frequent_df.iloc[:20,:], dodge=False, ax=ax)\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Toxic Comments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_POS_list = []\n",
    "\n",
    "for x in processed_df['POS_tagging_flat'][((processed_df.toxic == 0) &(processed_df.severe_toxic == 0) & (processed_df.obscene == 0) & (processed_df.threat == 0) & (processed_df.identity_hate == 0))]:\n",
    "    nontoxic_POS_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_POS_count = Counter(chain(*nontoxic_POS_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_POS_frequent_df = pd.DataFrame.from_dict(nontoxic_POS_count, orient='index').reset_index()\n",
    "nontoxic_POS_frequent_df.columns = ['nontoxic_POS_tags', 'Count']\n",
    "nontoxic_POS_frequent_df = nontoxic_POS_frequent_df.sort_values(by=['Count'], ascending=False)\n",
    "nontoxic_POS_frequent_df['Word'] = nontoxic_POS_frequent_df['nontoxic_POS_tags'].apply(lambda x: x[0])\n",
    "nontoxic_POS_frequent_df['Tag'] = nontoxic_POS_frequent_df['nontoxic_POS_tags'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_POS_frequent_df = nontoxic_POS_frequent_df.query(\"Word not in @stop_words & Word not in @list_punctuations\")\n",
    "nontoxic_POS_frequent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Top frequent non-toxic tags\", fontsize=12)\n",
    "sns.barplot(x='Count', y='Word', hue='Tag', \n",
    "            data=nontoxic_POS_frequent_df.iloc[:20,:], dodge=False, ax=ax)\n",
    "ax.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uba5KcyE6lO1"
   },
   "source": [
    "## Weighted Words - Bag of Words (BoW) - Bag of n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Words Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:24:33.386108Z",
     "start_time": "2023-03-15T04:24:33.382199Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "def most_frequent_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    fdist = FreqDist(words) \n",
    "    \n",
    "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
    "                             'Frequency': fdist.values()})\n",
    "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    return df_fdist.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords & punctuaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "processed_df['lemmatization'] = processed_df['lemmatization'].apply(lambda x: [ i for i in x if (i not in string.punctuation) & (i not in stop_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:24:40.139486Z",
     "start_time": "2023-03-15T04:24:35.827534Z"
    }
   },
   "outputs": [],
   "source": [
    "#identify frequent words\n",
    "\n",
    "combined_toxic = processed_df['lemmatization'][((processed_df.toxic == 1) | (processed_df.severe_toxic == 1) | (processed_df.obscene == 1) | (processed_df.threat == 1) | (processed_df.identity_hate == 1))].str.join(' ')\n",
    "combined_toxic = ' '.join(combined_toxic)\n",
    "toxic_frequent = most_frequent_words(combined_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:25:29.951118Z",
     "start_time": "2023-03-15T04:24:40.160729Z"
    }
   },
   "outputs": [],
   "source": [
    "#identify frequent words\n",
    "combined_nontoxic = processed_df['lemmatization'][((processed_df.toxic == 0) &(processed_df.severe_toxic == 0) & (processed_df.obscene == 0) & (processed_df.threat == 0) & (processed_df.identity_hate == 0))].str.join(' ')\n",
    "combined_nontoxic = ' '.join(combined_nontoxic)\n",
    "nontoxic_frequent = most_frequent_words(combined_nontoxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:25:29.981812Z",
     "start_time": "2023-03-15T04:25:29.974130Z"
    }
   },
   "outputs": [],
   "source": [
    "toxic_frequent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:25:30.009772Z",
     "start_time": "2023-03-15T04:25:29.998300Z"
    }
   },
   "outputs": [],
   "source": [
    "nontoxic_frequent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:25:30.408822Z",
     "start_time": "2023-03-15T04:25:30.027849Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='black',\n",
    "                          max_font_size = 80\n",
    "                         ).generate(\" \".join(toxic_frequent['Word']))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.title(\"Wordcloud for toxic comments\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non toxic comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:25:30.687489Z",
     "start_time": "2023-03-15T04:25:30.428328Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          max_font_size = 80,\n",
    "                          colormap='rainbow'\n",
    "                         ).generate(\" \".join(nontoxic_frequent['Word']))\n",
    "plt.imshow(word_cloud)\n",
    "plt.title(\"Wordcloud for non-toxic comments\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:32:43.156963Z",
     "start_time": "2023-03-15T04:32:18.070416Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(2, 2))\n",
    "bow = vec.fit_transform(processed_df['text_tokenized'].str.join(' '))\n",
    "sum_of_words = bow.sum(axis=0)\n",
    "bigrams_freq = [(word, sum_of_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "bigrams_freq =sorted(bigrams_freq, key = lambda x: x[1], reverse=True)\n",
    "bigrams_freq = pd.DataFrame(bigrams_freq[:50], columns=[\"bigrams\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:34:00.192829Z",
     "start_time": "2023-03-15T04:33:59.470751Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = bigrams_freq, x = \"frequency\", y=\"bigrams\")\n",
    "plt.title(\"Bigrams frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:36:36.846090Z",
     "start_time": "2023-03-15T04:35:51.957877Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3))\n",
    "bow = vec.fit_transform(processed_df['text_tokenized'].str.join(' '))\n",
    "sum_of_words = bow.sum(axis=0)\n",
    "trigrams_freq = [(word, sum_of_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "trigrams_freq =sorted(trigrams_freq, key = lambda x: x[1], reverse=True)\n",
    "trigrams_freq = pd.DataFrame(trigrams_freq[:50], columns=[\"trigrams\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T04:36:40.109311Z",
     "start_time": "2023-03-15T04:36:39.322480Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = trigrams_freq, x = \"frequency\", y=\"trigrams\")\n",
    "plt.title(\"Trigrams frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcKLU4DFQ-tA"
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf =  TfidfVectorizer(ngram_range=(2, 2))\n",
    "bow = tfidf.fit_transform(processed_df['text_tokenized'].str.join(' '))\n",
    "sum_of_words = bow.sum(axis=0)\n",
    "bigrams_freq = [(word, sum_of_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
    "bigrams_freq =sorted(bigrams_freq, key = lambda x: x[1], reverse=True)\n",
    "bigrams_freq = pd.DataFrame(bigrams_freq[:50], columns=[\"bigrams\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = bigrams_freq, x = \"frequency\", y=\"bigrams\")\n",
    "plt.title(\"Bigrams frequency in TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf =  TfidfVectorizer(ngram_range=(3, 3))\n",
    "bow = tfidf.fit_transform(processed_df['text_tokenized'].str.join(' '))\n",
    "sum_of_words = bow.sum(axis=0)\n",
    "trigrams_freq = [(word, sum_of_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
    "trigrams_freq =sorted(trigrams_freq, key = lambda x: x[1], reverse=True)\n",
    "trigrams_freq = pd.DataFrame(trigrams_freq[:50], columns=[\"trigrams\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = trigrams_freq, x = \"frequency\", y=\"trigrams\")\n",
    "plt.title(\"Trigrams frequency in TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oversampled= pd.read_csv(\"Data/train_oversampled.csv\")\n",
    "train_oversampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#calculate VADER score\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "semtiment_df = processed_df[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate','lemmatization']]\n",
    "semtiment_df[\"Positive\"] = [sentiments.polarity_scores(\" \".join(i))[\"pos\"] for i in semtiment_df[\"lemmatization\"]]\n",
    "semtiment_df[\"Negative\"] = [sentiments.polarity_scores(\" \".join(i))[\"neg\"] for i in semtiment_df[\"lemmatization\"]]\n",
    "semtiment_df[\"Neutral\"] = [sentiments.polarity_scores(\" \".join(i))[\"neu\"] for i in semtiment_df[\"lemmatization\"]]\n",
    "semtiment_df[\"compound\"] = [sentiments.polarity_scores(\" \".join(i))[\"compound\"] for i in semtiment_df[\"lemmatization\"]]\n",
    "# semtiment_df = processed_df[[\"content\", \"Positive\", \"Negative\", \"Neutral\"]]\n",
    "semtiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_pie_count_list = []\n",
    "senti_pie_count_list.append(len(semtiment_df[semtiment_df['compound'] < -0.05]))\n",
    "senti_pie_count_list.append(len(semtiment_df[semtiment_df['compound'] > 0.05]))\n",
    "senti_pie_count_list.append(len(semtiment_df[(semtiment_df['compound'] < 0.05) & (semtiment_df['compound'] > -0.05)]))\n",
    "\n",
    "colors = ['pink', 'silver', 'steelblue']\n",
    "labels = [\"Negative\",\"Positive\",\"Neutral\"]\n",
    "\n",
    "plt.pie(np.array(senti_pie_count_list),labels = labels, colors = colors,autopct='%1.1f%%')\n",
    "plt.title(\"Number of tweets in each sentiment category\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words =' '.join([\" \".join(i) for i in semtiment_df['lemmatization'][semtiment_df['compound'] > 0.05]])\n",
    "# stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color=\"white\",stopwords=set(STOPWORDS),collocations=False).generate(positive_words)\n",
    "plt.figure( figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Frequent words in positive tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "negative_words =' '.join([\" \".join(i) for i in semtiment_df['lemmatization'][semtiment_df['compound'] < -0.05]])\n",
    "# stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color=\"white\",colormap = 'inferno',stopwords=set(STOPWORDS),collocations=False).generate(negative_words)\n",
    "plt.figure( figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Frequent words in negative tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test relationship between negative VADER score with toxic comments\n",
    "semtiment_df['negative']= np.where(semtiment_df['compound'] < -0.05, 1, 0)\n",
    "contigency= pd.crosstab(semtiment_df['negative'], semtiment_df['toxic'])\n",
    "contigency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(contigency, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "# Chi-square test of independence\n",
    "# H0 : the variables are independent, there is no relationship between the two categorical variables\n",
    "# H1 : the variables are dependent, there is a relationship between the two categorical variables.\n",
    "c, p, dof, expected = chi2_contingency(contigency)\n",
    "if p < 0.05:\n",
    "    print(\"p < alpha, reject H0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featureengineering\"></a>\n",
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some decisions we need to make:\n",
    "- should stop words and punctuations be removed before embedding\n",
    "- custom embeddings or pre-trained embeddings or embedding layer\n",
    "- for pre-trained embeddings, which package to use: Word2Vec/GloVe, FastText, BERT\n",
    "- the size of vector to represent each text\n",
    "- word embedding or sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word embeddings\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# for BERT\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to calculate the embedding vector of each text\n",
    "def get_embeddings(vectors, text_df, generate_missing=False, k=25):\n",
    "    embeddings = []\n",
    "    for i in range(len(text_df)):\n",
    "        text = text_df[i]\n",
    "        # dealing with empty text\n",
    "        if len(text)<1:\n",
    "            return np.zeros(k)\n",
    "        # generate randomized vectors for unseen words if generate_missing is True\n",
    "        if generate_missing:\n",
    "            vectorized = [vectors[word][:k] if word in vectors else np.random.rand(k) for word in text]\n",
    "        # represent unseen words with 0 vector if generate_missing is False\n",
    "        else:\n",
    "            vectorized = [vectors[word][:k] if word in vectors else np.zeros(k) for word in text]\n",
    "        # each text is represented by averaging the vectors of its constituent words\n",
    "        length = len(vectorized)\n",
    "        summed = np.sum(vectorized, axis=0)\n",
    "        averaged = np.divide(summed, length)\n",
    "        embeddings.append(averaged)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to calculate the embedding vector of each text\n",
    "def get_embeddings(vectors, text, generate_missing=False, k=25):\n",
    "    # dealing with empty text\n",
    "    if len(text)<1:\n",
    "        return np.zeros(k)\n",
    "    # generate randomized vectors for unseen words if generate_missing is True\n",
    "    if generate_missing:\n",
    "        vectorized = [vectors[word][:k] if word in vectors else np.random.rand(k) for word in text]\n",
    "    # represent unseen words with 0 vector if generate_missing is False\n",
    "    else:\n",
    "        vectorized = [vectors[word][:k] if word in vectors else np.zeros(k) for word in text]\n",
    "    # each text is represented by averaging the vectors of its constituent words\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Vectors for Word Representation (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Word2Vec method, each word is presented by a high dimension vector and trained based on the surrounding words over a huge corpus. Since our project works on Tweets, we apply a pre-trained word embedding trained over Twitter content (https://github.com/stanfordnlp/GloVe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- It captures the position of the words in the text (syntactic)\n",
    "- It captures meaning in the words (semantics)\n",
    "\n",
    "Limitations:\n",
    "- It cannot capture the meaning of the word from the text (fails to capture polysemy)\n",
    "- It cannot capture out-of-vocabulary words from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a word2vec file used for model building\n",
    "glove_input_file = \"D:/GoogleDownloads/BT4222/glove.twitter.27B/glove.twitter.27B.25d.txt\"\n",
    "word2vec_output_file = \"glove.twitter.27B.25d.txt.word2vec\"\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a GloVe model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding vectors of size 25 using tokenized text (stop words and punctuation kept)\n",
    "# side note: map function outperforms for loop\n",
    "embeddings_glove = processed_df[\"text_tokenized\"].map(lambda x: get_embeddings(glove_model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result of embeddings\n",
    "fig, axes = plt.subplots(nrows = 5, ncols=5, figsize=(15,15))\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for dim in range(25):\n",
    "    for label in labels:\n",
    "        idx = processed_df[processed_df[label]==1].index.values\n",
    "        li = list(map(lambda x: embeddings_glove[x][dim], idx))\n",
    "        sns.kdeplot(li, fill=True, ax=axes[dim//5,dim%5])\n",
    "    idx = processed_df[processed_df[labels].sum(axis=1)==0].index.values\n",
    "    li = list(map(lambda x: embeddings_glove[x][dim], idx))\n",
    "    sns.kdeplot(li, fill=True, ax=axes[dim//5,dim%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference among different labels is not very obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embeddings_glove).to_csv('embeddings_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FastText, each word, w, is represented as a bag of character n-gram. For example, given the word \"introduce\" and n = 3, FastText will produce the following representation composed of character tri-grams: < in, int, ntr, tro, rod, odu, duc, uce, ce > Note that the sequence, corresponding to the word here is different from the tri-gram \"int\" from the word introduce. Again, FastText is pre-trained on huge corpus (https://fasttext.cc/docs/en/english-vectors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Works for rare words, if their character n-grams which are still shared with other words\n",
    "- Solves out of vocabulary words with n-gram in character level\n",
    "\n",
    "Limitations:\n",
    "- It cannot capture the meaning of the word from the text (fails to capture polysemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a FastText model\n",
    "start_time3 = time.time()\n",
    "fasttext_path = \"D:/GoogleDownloads/BT4222/crawl-300d-2M-subword/crawl-300d-2M-subword.vec\"\n",
    "fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_path, binary=False, limit=30000000000) # load 30B most common words\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding vectors of size 25 using tokenized text (stop words and punctuation kept)\n",
    "embeddings_fasttext = processed_df[\"text_tokenized\"].map(lambda x: get_embeddings(fasttext_model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result of embeddings\n",
    "fig, axes = plt.subplots(nrows = 5, ncols=5, figsize=(15,15))\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for dim in range(25):\n",
    "    for label in labels:\n",
    "        idx = processed_df[processed_df[label]==1].index.values\n",
    "        li = list(map(lambda x: embeddings_fasttext[x][dim], idx))\n",
    "        sns.kdeplot(li, fill=True, ax=axes[dim//5,dim%5])\n",
    "    idx = processed_df[processed_df[labels].sum(axis=1)==0].index.values\n",
    "    li = list(map(lambda x: embeddings_fasttext[x][dim], idx))\n",
    "    sns.kdeplot(li, fill=True, ax=axes[dim//5,dim%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in distribution among different labels is very slight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embeddings_fasttext).to_csv('embeddings_fasttext.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While each word has a fixed representation under forementioned models regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them. We use the BERT base model to create sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "-  It captures the meaning of the word from the text (incorporates context, handling polysemy)\n",
    "\n",
    "Limitations:\n",
    "- Computationally expensive\n",
    "- It cannot capture out-of-vocabulary words from a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to calculate the embedding vector of each text\n",
    "def get_BERT_embeddings(text, max_len=512): # BERT can only take up to 512 tokens\n",
    "    # tokenize sentence with BERT tokenizer\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text = tokenized_text[:max_len-2] # truncate overlength text\n",
    "    # add special tokens\n",
    "    tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "    # map the token strings to their vocabulary indeces\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # create segment ID\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    # convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    # fead the inputs into BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "    # use the average of the second last hidden layer of each token as sentence embedding\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0).numpy()\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True)# whether the model returns all hidden-states\n",
    "# put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input used for BERT model\n",
    "sentence_tokenized = processed_df['text_tokenized'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate embedding vectors (stop words and punctuation kept)\n",
    "start_time4 = time.time()\n",
    "embeddings_bert = sentence_tokenized.map(get_BERT_embeddings)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result of embeddings\n",
    "fig, axes = plt.subplots(nrows = 5, ncols=5, figsize=(15,15))\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for dim in range(25):\n",
    "    for label in labels:\n",
    "        idx = processed_df[processed_df[label]==1].index.values\n",
    "        li = list(map(lambda x: embeddings_bert[x][dim], idx))\n",
    "        sns.kdeplot(li, fill=True, ax=axes[dim//5,dim%5])\n",
    "    idx = processed_df[processed_df[labels].sum(axis=1)==0].index.values\n",
    "    li = list(map(lambda x: embeddings_bert[x][dim], idx))\n",
    "    sns.kdeplot(li, fill=True, ax=axes[dim//5,dim%5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embeddings_bert).to_csv('embeddings_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curseword Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://github.com/vzhou842/profanity-check to use as an indicator for curseword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df['curseWord'] =  featured_df['clean_text'].apply(lambda x: predict([x])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique, Repated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T08:38:05.535613Z",
     "start_time": "2023-03-21T08:38:04.187139Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df = processed_df.iloc[:,1:8]\n",
    "#clean_df['text_tokenized'] = clean_df['text_tokenized'].apply(lambda x: [c for c in x if c not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T10:03:14.977551Z",
     "start_time": "2023-03-21T10:03:14.921536Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#Without removing stopwords but punctuation\n",
    "\n",
    "def freq(text):\n",
    "    text = [c for c in text if c not in string.punctuation]\n",
    "    if len(text) == 0:\n",
    "        return [0,0,0,0,0,0,0,0]\n",
    "    try:\n",
    "        #get frequency vectore\n",
    "        vec = CountVectorizer(ngram_range=(1, 1), stop_words=None)\n",
    "        bow = vec.fit_transform([' '.join(text)])\n",
    "    except:\n",
    "        print(text)\n",
    "        #get frequency vectore\n",
    "        vec = CountVectorizer(ngram_range=(1, 1), stop_words=None)\n",
    "        bow = vec.fit_transform([''.join(text)])\n",
    "        print(bow)\n",
    "    finally:          \n",
    "        sum_of_words = bow.sum(axis=0)\n",
    "        unigrams_freq = [(word, sum_of_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "        #how many words\n",
    "        words = len(unigrams_freq)\n",
    "        unigrams_freq = pd.DataFrame(unigrams_freq,columns=['word','freq'])\n",
    "\n",
    "        # length of text\n",
    "        total_length = len(''.join(unigrams_freq.word))\n",
    "\n",
    "        #length of repeated text\n",
    "        repeated_length = len(''.join(unigrams_freq['word'][unigrams_freq.freq>1]))\n",
    "        #length of repeated text\n",
    "        unique_length = total_length-repeated_length\n",
    "\n",
    "        #how many repeated words\n",
    "        repeated_words = unigrams_freq[unigrams_freq.freq>1]\n",
    "        repeated_words_length = len(unigrams_freq[unigrams_freq.freq>1])\n",
    "\n",
    "        #how many unqiue words\n",
    "        unique_words_length = words - repeated_words_length\n",
    "\n",
    "\n",
    "        repeated_words_vs_length = repeated_length / total_length\n",
    "        repeated_words_vs_words = repeated_words_length / words\n",
    "        unique_words_vs_length = unique_length / total_length\n",
    "        unique_words_vs_words = unique_words_length / words\n",
    "        return [words, total_length, repeated_words_length, repeated_words_vs_length, repeated_words_vs_words,\n",
    "                 unique_words_length,unique_words_vs_length, unique_words_vs_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T10:09:02.762594Z",
     "start_time": "2023-03-21T10:03:17.541419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'i', 'g', 'g', 'e', 'r', 'f', 'a', 'g', 'g', 'o', 't']\n",
      "  (0, 0)\t1\n",
      "['p', 'e', 'n', 'i', 's']\n",
      "  (0, 0)\t1\n",
      "['p', 'e', 'n', 'i', 's']\n",
      "  (0, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "clean_df['words'],clean_df['total_length'],clean_df['repeated_words_length'],\\\n",
    "clean_df['repeated_words_vs_length'],clean_df['repeated_words_vs_words'],\\\n",
    "clean_df['unique_words_length'],clean_df['unique_words_vs_length'],\\\n",
    "clean_df['unique_words_vs_words'] = zip(*clean_df['text_tokenized'].apply(\n",
    "    lambda comment: freq(comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T10:09:12.899061Z",
     "start_time": "2023-03-21T10:09:12.889061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate',\n",
       "       'text_tokenized', 'words', 'total_length', 'repeated_words_vs_length',\n",
       "       'repeated_words_vs_words', 'unique_words_vs_length',\n",
       "       'unique_words_vs_words', 'repeated_words_length',\n",
       "       'unique_words_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T10:09:39.854036Z",
     "start_time": "2023-03-21T10:09:39.764434Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "features = ('words', 'total_length', 'repeated_words_vs_length',\n",
    "       'repeated_words_vs_words', 'unique_words_vs_length',\n",
    "       'unique_words_vs_words', 'repeated_words_length',\n",
    "       'unique_words_length'\n",
    "           )\n",
    "clean_df['none'] = 1 - clean_df[LABELS].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T10:09:49.797057Z",
     "start_time": "2023-03-21T10:09:49.553062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>-0.099039</td>\n",
       "      <td>-0.050574</td>\n",
       "      <td>-0.083467</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>-0.083366</td>\n",
       "      <td>-0.033696</td>\n",
       "      <td>0.094470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_length</th>\n",
       "      <td>-0.098959</td>\n",
       "      <td>-0.047987</td>\n",
       "      <td>-0.083478</td>\n",
       "      <td>-0.022275</td>\n",
       "      <td>-0.082849</td>\n",
       "      <td>-0.031375</td>\n",
       "      <td>0.095204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repeated_words_vs_length</th>\n",
       "      <td>-0.016289</td>\n",
       "      <td>0.048949</td>\n",
       "      <td>-0.011058</td>\n",
       "      <td>0.012372</td>\n",
       "      <td>-0.008599</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.015832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repeated_words_vs_words</th>\n",
       "      <td>-0.031051</td>\n",
       "      <td>0.034609</td>\n",
       "      <td>-0.026098</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>-0.021725</td>\n",
       "      <td>-0.001735</td>\n",
       "      <td>0.029628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_words_vs_length</th>\n",
       "      <td>0.016496</td>\n",
       "      <td>-0.048752</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>-0.012300</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>-0.005101</td>\n",
       "      <td>-0.016047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_words_vs_words</th>\n",
       "      <td>0.031218</td>\n",
       "      <td>-0.034470</td>\n",
       "      <td>0.026212</td>\n",
       "      <td>-0.008254</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>-0.029806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repeated_words_length</th>\n",
       "      <td>-0.085243</td>\n",
       "      <td>-0.035968</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>-0.016286</td>\n",
       "      <td>-0.070045</td>\n",
       "      <td>-0.027245</td>\n",
       "      <td>0.081918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_words_length</th>\n",
       "      <td>-0.102455</td>\n",
       "      <td>-0.055957</td>\n",
       "      <td>-0.087231</td>\n",
       "      <td>-0.022547</td>\n",
       "      <td>-0.087064</td>\n",
       "      <td>-0.035704</td>\n",
       "      <td>0.097437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             toxic  severe_toxic   obscene    threat  \\\n",
       "words                    -0.099039     -0.050574 -0.083467 -0.020974   \n",
       "total_length             -0.098959     -0.047987 -0.083478 -0.022275   \n",
       "repeated_words_vs_length -0.016289      0.048949 -0.011058  0.012372   \n",
       "repeated_words_vs_words  -0.031051      0.034609 -0.026098  0.008308   \n",
       "unique_words_vs_length    0.016496     -0.048752  0.011210 -0.012300   \n",
       "unique_words_vs_words     0.031218     -0.034470  0.026212 -0.008254   \n",
       "repeated_words_length    -0.085243     -0.035968 -0.070000 -0.016286   \n",
       "unique_words_length      -0.102455     -0.055957 -0.087231 -0.022547   \n",
       "\n",
       "                            insult  identity_hate      none  \n",
       "words                    -0.083366      -0.033696  0.094470  \n",
       "total_length             -0.082849      -0.031375  0.095204  \n",
       "repeated_words_vs_length -0.008599       0.005186  0.015832  \n",
       "repeated_words_vs_words  -0.021725      -0.001735  0.029628  \n",
       "unique_words_vs_length    0.008750      -0.005101 -0.016047  \n",
       "unique_words_vs_words     0.021842       0.001798 -0.029806  \n",
       "repeated_words_length    -0.070045      -0.027245  0.081918  \n",
       "unique_words_length      -0.087064      -0.035704  0.097437  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = LABELS + ['none']\n",
    "\n",
    "rows = [{c:clean_df[f].corr(clean_df[c]) for c in columns} for f in features]\n",
    "train_correlations_unique = pd.DataFrame(rows, index=features)\n",
    "train_correlations_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns, Verbs, Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "pos_analysis_df = train_df.copy()\n",
    "\n",
    "def tag_part_of_speech(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    pos_list = pos_tag(text_splited)\n",
    "    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    adjective_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    verb_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    return[noun_count, adjective_count, verb_count]\n",
    "\n",
    "\n",
    "pos_analysis_df['nouns'], pos_analysis_df['adjectives'], pos_analysis_df['verbs'] = zip(*pos_analysis_df['comment_text'].apply(\n",
    "    lambda comment: tag_part_of_speech(comment)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_analysis_df['total_length'] = pos_analysis_df['comment_text'].apply(len)\n",
    "\n",
    "pos_analysis_df['words'] = pos_analysis_df['comment_text'].apply(lambda comment: len(comment.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_analysis_df['nouns_vs_length'] = pos_analysis_df['nouns'] / pos_analysis_df['total_length']\n",
    "pos_analysis_df['adjectives_vs_length'] = pos_analysis_df['adjectives'] / pos_analysis_df['total_length']\n",
    "pos_analysis_df['verbs_vs_length'] = pos_analysis_df['verbs'] / pos_analysis_df['total_length']\n",
    "pos_analysis_df['nouns_vs_words'] = pos_analysis_df['nouns'] / pos_analysis_df['words']\n",
    "pos_analysis_df['adjectives_vs_words'] = pos_analysis_df['adjectives'] / pos_analysis_df['words']\n",
    "pos_analysis_df['verbs_vs_words'] = pos_analysis_df['verbs'] / pos_analysis_df['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>verbs</th>\n",
       "      <th>total_length</th>\n",
       "      <th>words</th>\n",
       "      <th>nouns_vs_length</th>\n",
       "      <th>adjectives_vs_length</th>\n",
       "      <th>verbs_vs_length</th>\n",
       "      <th>nouns_vs_words</th>\n",
       "      <th>adjectives_vs_words</th>\n",
       "      <th>verbs_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>264</td>\n",
       "      <td>43</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.186047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>17</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>233</td>\n",
       "      <td>42</td>\n",
       "      <td>0.047210</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.030043</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>622</td>\n",
       "      <td>113</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.036977</td>\n",
       "      <td>0.212389</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.203540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>13</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  nouns  adjectives  \\\n",
       "0             0        0       0       0              0     17           2   \n",
       "1             0        0       0       0              0      7           0   \n",
       "2             0        0       0       0              0     11           2   \n",
       "3             0        0       0       0              0     24           6   \n",
       "4             0        0       0       0              0      4           0   \n",
       "\n",
       "   verbs  total_length  words  nouns_vs_length  adjectives_vs_length  \\\n",
       "0      8           264     43         0.064394              0.007576   \n",
       "1      3           112     17         0.062500              0.000000   \n",
       "2      7           233     42         0.047210              0.008584   \n",
       "3     23           622    113         0.038585              0.009646   \n",
       "4      3            67     13         0.059701              0.000000   \n",
       "\n",
       "   verbs_vs_length  nouns_vs_words  adjectives_vs_words  verbs_vs_words  \n",
       "0         0.030303        0.395349             0.046512        0.186047  \n",
       "1         0.026786        0.411765             0.000000        0.176471  \n",
       "2         0.030043        0.261905             0.047619        0.166667  \n",
       "3         0.036977        0.212389             0.053097        0.203540  \n",
       "4         0.044776        0.307692             0.000000        0.230769  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "features = (\n",
    "            'nouns', 'nouns_vs_words', 'nouns_vs_length', \n",
    "            'adjectives', 'adjectives_vs_words', 'adjectives_vs_length',\n",
    "            'verbs', 'verbs_vs_words', 'verbs_vs_length',\n",
    "           )\n",
    "pos_analysis_df['none'] = 1 - pos_analysis_df[LABELS].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nouns</th>\n",
       "      <td>-0.000861</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>-0.001149</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>-0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nouns_vs_words</th>\n",
       "      <td>0.125599</td>\n",
       "      <td>0.113773</td>\n",
       "      <td>0.121616</td>\n",
       "      <td>0.021363</td>\n",
       "      <td>0.110377</td>\n",
       "      <td>0.066107</td>\n",
       "      <td>-0.121250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nouns_vs_length</th>\n",
       "      <td>0.178798</td>\n",
       "      <td>0.140118</td>\n",
       "      <td>0.168074</td>\n",
       "      <td>0.036472</td>\n",
       "      <td>0.153487</td>\n",
       "      <td>0.085114</td>\n",
       "      <td>-0.174803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjectives</th>\n",
       "      <td>-0.045447</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>-0.036740</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.032686</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.042934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjectives_vs_words</th>\n",
       "      <td>0.028471</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.016104</td>\n",
       "      <td>-0.011183</td>\n",
       "      <td>0.030969</td>\n",
       "      <td>0.034752</td>\n",
       "      <td>-0.029019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjectives_vs_length</th>\n",
       "      <td>0.047081</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>-0.005735</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.043044</td>\n",
       "      <td>-0.047881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verbs</th>\n",
       "      <td>-0.061767</td>\n",
       "      <td>-0.014051</td>\n",
       "      <td>-0.051171</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>-0.051491</td>\n",
       "      <td>-0.021670</td>\n",
       "      <td>0.057725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verbs_vs_words</th>\n",
       "      <td>-0.004188</td>\n",
       "      <td>-0.032360</td>\n",
       "      <td>-0.011213</td>\n",
       "      <td>0.007587</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>-0.013781</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verbs_vs_length</th>\n",
       "      <td>0.024287</td>\n",
       "      <td>-0.019703</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>-0.005127</td>\n",
       "      <td>-0.026003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         toxic  severe_toxic   obscene    threat    insult  \\\n",
       "nouns                -0.000861      0.068455  0.001967  0.003365 -0.001149   \n",
       "nouns_vs_words        0.125599      0.113773  0.121616  0.021363  0.110377   \n",
       "nouns_vs_length       0.178798      0.140118  0.168074  0.036472  0.153487   \n",
       "adjectives           -0.045447      0.004472 -0.036740 -0.015309 -0.032686   \n",
       "adjectives_vs_words   0.028471      0.005824  0.016104 -0.011183  0.030969   \n",
       "adjectives_vs_length  0.047081      0.012349  0.032990 -0.005735  0.046271   \n",
       "verbs                -0.061767     -0.014051 -0.051171 -0.003157 -0.051491   \n",
       "verbs_vs_words       -0.004188     -0.032360 -0.011213  0.007587 -0.008274   \n",
       "verbs_vs_length       0.024287     -0.019703  0.013053  0.020133  0.011791   \n",
       "\n",
       "                      identity_hate      none  \n",
       "nouns                      0.009624 -0.000103  \n",
       "nouns_vs_words             0.066107 -0.121250  \n",
       "nouns_vs_length            0.085114 -0.174803  \n",
       "adjectives                -0.000146  0.042934  \n",
       "adjectives_vs_words        0.034752 -0.029019  \n",
       "adjectives_vs_length       0.043044 -0.047881  \n",
       "verbs                     -0.021670  0.057725  \n",
       "verbs_vs_words            -0.013781  0.002575  \n",
       "verbs_vs_length           -0.005127 -0.026003  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = LABELS + ['none']\n",
    "\n",
    "rows = [{c:pos_analysis_df[f].corr(pos_analysis_df[c]) for c in columns} for f in features]\n",
    "train_correlations = pd.DataFrame(rows, index=features)\n",
    "train_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.818px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
